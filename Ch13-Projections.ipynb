{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 13: Projections and Orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: pp. 363 - 388\n",
    "- exercists: pp. 389 - 394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "The goal of this chapter is to introduce a framework for projecting one space onto another space (e.g. a 3D shape forming a 2D shadow).  This framework forms the basis for orthogonalization and for an algorithm called *linear least-squares*, which is the primary method for estimating parameters and fitting models to data, and is therefore one of the most important algorithms in applied mathematics, including control engineering, statistics, and machine learning.  Along the way, we'll also rediscover the left inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Projections in $\\mathbb{R}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to discover a formula for projecting a point onto a line, and then generalize that formula to other projections.\n",
    "- start with:\n",
    "  - a vector $a$\n",
    "  - point $b$ not on $a$\n",
    "  - scalar $\\beta$ such that $\\beta a$ is as close to $b$ as possible without leaving $a$\n",
    "- the question is, where do we place $\\beta$ so that the point $\\beta a$ is as close as possible to point $b$?\n",
    "- the answer: when the line from $\\beta a$ to $b$ is at a right angle to $a$. (i.e. orthogonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can express the line from point $b$ to point $\\beta a$ as a subtraction from vector $b$.\n",
    "- thus, the expression for the line is $b - \\beta a$\n",
    "- Importantly, vectors $a$ and $(b - \\beta a)$ are orthogonal / perpendicular\n",
    "$$(b - \\beta a) \\perp a$$\n",
    "- and since they are orthogonal, that means that the dot product between them is 0, so we can rewrite the equation as:\n",
    "$$(b - \\beta a)^T a = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from here, we can use algebra to solve for $\\beta$\n",
    "$$a^T(b - \\beta a) = 0$$\n",
    "$$a^Tb - \\beta a^Ta = 0$$\n",
    "$$\\beta a^Ta = a^Tb$$\n",
    "$$\\beta = \\frac{a^Tb}{a^Ta}$$\n",
    "*(note that dividing both sides by $a^Ta$ is valid because it is a scalar)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "- Projections of $b$ onto the subspace defined by vector $a$ is typically written as\n",
    "$$proj_a(b)$$\n",
    "- note that it can be tricky to remember which is projecting onto which in $proj_a(b)$ or $proj_b(a)$\n",
    "  - a memory trick is that the **Subspace goes in the Subscript**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation for the projection of a point onto a line\n",
    "$$proj_a(b) = \\frac{a^Tb}{a^Ta}a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = \n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix},\n",
    "b = (3, -1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "proj_a(b) = \n",
    "\\frac\n",
    "{\n",
    "  \\begin{bmatrix}\n",
    "  -2 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}^T\n",
    "  \\begin{bmatrix}\n",
    "  3 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}\n",
    "}\n",
    "{\n",
    "  \\begin{bmatrix}\n",
    "  -2 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}^T\n",
    "  \\begin{bmatrix}\n",
    "  -2 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}\n",
    "}\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\frac{-6 + 1}{4 + 1}\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "-1\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that $\\beta = -1$\n",
    "- thus, we are projecting \"backwards\" onto the vector.\n",
    "- this makes sense when we think of $a$ as being a basis vector for a 1D subspace that is embedded in $\\mathbb{R}^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "*Mapping over magnitude:*  Meditating on the projection equation will reveal that it is a mapping between two vectors, scaled by the squared length of the \"target\" vector.  It's useful to understand this intuition (mapping over magnitude), because many computations in linear algebra and its applications (e.g. correlation, convolution, normalization) involve some kind of mapping divided by some kind of magnitude or norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Projections in $\\mathbb{R}^N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Orth and par vect comps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 Orthogonalization via GS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.6 QR decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.7 Inverse via QR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.8 - 13.9 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.10 - 13.11 Code challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
