{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 13: Projections and Orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: pp. 363 - 388\n",
    "- exercists: pp. 389 - 394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "The goal of this chapter is to introduce a framework for projecting one space onto another space (e.g. a 3D shape forming a 2D shadow).  This framework forms the basis for orthogonalization and for an algorithm called *linear least-squares*, which is the primary method for estimating parameters and fitting models to data, and is therefore one of the most important algorithms in applied mathematics, including control engineering, statistics, and machine learning.  Along the way, we'll also rediscover the left inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Projections in $\\mathbb{R}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to discover a formula for projecting a point onto a line, and then generalize that formula to other projections.\n",
    "- start with:\n",
    "  - a vector $a$\n",
    "  - point $b$ not on $a$\n",
    "  - scalar $\\beta$ such that $\\beta a$ is as close to $b$ as possible without leaving $a$\n",
    "- the question is, where do we place $\\beta$ so that the point $\\beta a$ is as close as possible to point $b$?\n",
    "- the answer: when the line from $\\beta a$ to $b$ is at a right angle to $a$. (i.e. orthogonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can express the line from point $b$ to point $\\beta a$ as a subtraction from vector $b$.\n",
    "- thus, the expression for the line is $b - \\beta a$\n",
    "- Importantly, vectors $a$ and $(b - \\beta a)$ are orthogonal / perpendicular\n",
    "$$(b - \\beta a) \\perp a$$\n",
    "- and since they are orthogonal, that means that the dot product between them is 0, so we can rewrite the equation as:\n",
    "$$(b - \\beta a)^T a = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from here, we can use algebra to solve for $\\beta$\n",
    "$$a^T(b - \\beta a) = 0$$\n",
    "$$a^Tb - \\beta a^Ta = 0$$\n",
    "$$\\beta a^Ta = a^Tb$$\n",
    "$$\\beta = \\frac{a^Tb}{a^Ta}$$\n",
    "*(note that dividing both sides by $a^Ta$ is valid because it is a scalar)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "- Projections of $b$ onto the subspace defined by vector $a$ is typically written as\n",
    "$$proj_a(b)$$\n",
    "- note that it can be tricky to remember which is projecting onto which in $proj_a(b)$ or $proj_b(a)$\n",
    "  - a memory trick is that the **Subspace goes in the Subscript**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation for the projection of a point onto a line\n",
    "$$proj_a(b) = \\frac{a^Tb}{a^Ta}a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = \n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix},\n",
    "b = (3, -1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "proj_a(b) = \n",
    "\\frac\n",
    "{\n",
    "  \\begin{bmatrix}\n",
    "  -2 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}^T\n",
    "  \\begin{bmatrix}\n",
    "  3 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}\n",
    "}\n",
    "{\n",
    "  \\begin{bmatrix}\n",
    "  -2 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}^T\n",
    "  \\begin{bmatrix}\n",
    "  -2 \\\\\n",
    "  -1\n",
    "  \\end{bmatrix}\n",
    "}\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\frac{-6 + 1}{4 + 1}\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "-1\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that $\\beta = -1$\n",
    "- thus, we are projecting \"backwards\" onto the vector.\n",
    "- this makes sense when we think of $a$ as being a basis vector for a 1D subspace that is embedded in $\\mathbb{R}^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "*Mapping over magnitude:*  Meditating on the projection equation will reveal that it is a mapping between two vectors, scaled by the squared length of the \"target\" vector.  It's useful to understand this intuition (mapping over magnitude), because many computations in linear algebra and its applications (e.g. correlation, convolution, normalization) involve some kind of mapping divided by some kind of magnitude or norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Projections in $\\mathbb{R}^N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that we can project a point onto a line, we're going to extend this by projecting a point onto any dimensional subspace\n",
    "- we begin by replacing vector $a$ (which is a 1D subspace) with matrix $A$, the columns of which form a subspace with some dimensionality betweeen 1 and the matrix rank.\n",
    "- point $b$ is still the same.\n",
    "- Because $A$ has multiple columns, we also need to replace scalar $\\beta$ with a vector.  We'll call that vector $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A^T(b-Ax)=0$$\n",
    "$$A^Tb - A^TAx=0$$\n",
    "$$A^TAx=A^Tb$$\n",
    "- unlike before, we can't divide by $A^TA$ because it is a matrix\n",
    "- So instead, we'll left multiply by the inverse (since that is essentially dividing by a matrix)\n",
    "$$x=(A^TA)^{-1} A^Tb$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note thta $A^TA$ must be square and full rank for this equation to be valid\n",
    "- $A^TA$ is always square (so that is given)\n",
    "- $A^TA$ is full rank only when $A$ is full column rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.5,  4. ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate least squares in Python\n",
    "A = [[1,2],[3,1],[1,1]]\n",
    "b = [5.5, -3.5, 1.5]\n",
    "np.linalg.lstsq(A,b, rcond=None)[0]   # note: a \"FutureWarning popped up recommending to add `rcond=None`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Orthogonal and parallel vector components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this section, we'll learn how to decompose one vector into two separate vectors that are orthogonal to each other, and that have a special relationship to a 3rd vector.\n",
    "- Begin with an example:\n",
    "  - start with 2 vectors: $w$ (target vector), and $v$ (reference vector)\n",
    "  - we want to break up $w$ into two separate vectors, one *parallel* to $v$, and the other *perpendicular* to $v$\n",
    "    - the component of $w$ that is parallel to $v$ is labeled $w_{||v}$\n",
    "    - the component of $w$ that is perpendicular to $v$ is labeled $w_{\\perp v}$\n",
    "  - these two components sum to form w.  In other words:\n",
    "$$w = w_{|| v} + w_{\\perp v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if w and v have their tails at the same point, then the component of w that is parallel to v is simply collinear with v.\n",
    "- in other words, $w_{||v}$ is a scaled version of v.\n",
    "- If we think about it, this is exactly what we did previously with projections!  So let's grab the formula and use it here.\n",
    "$$w_{||v} = proj_v(w) = \\frac{w^Tv}{w^Tv}v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perpendicular component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to solve for $w_{\\perp v}$, recall that the vector w is the sum of the parallel and perpendicular components\n",
    "$$w_{\\perp v} = w - w_{||v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of equations decomposing a vector relative to another vector\n",
    "$$w = w_{|| v} + w_{\\perp v}$$\n",
    "$$w_{||v} = proj_v(w) = \\frac{w^Tv}{w^Tv}v$$\n",
    "$$w_{\\perp v} = w - w_{||v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip to remember the formulas:\n",
    "- because $w_{||v}$ is parallel to v, it's really the same as vector v but scaled.\n",
    "- thus $w_{||v} = \\alpha v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "The geometric representations easily demonstrate that the algebra is correct.  Whenever possible, you should learn math by solving problems where you can *see* the correct answer, and working on more challenging problems only after understanding the concept and algorithm in teh visualizable examples.  The same principle underlies the motivation to test statistical analysis methods on simulated data (where the ground truth is known) before applying those methods to empirical data (where ground truth is usually unknown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- typically indicated using the letter $Q$\n",
    "- (not the only letter used for orthogonal matrices, but if you see the letter $Q$ for a matrix, its safe to assume its orthogonal.)\n",
    "\n",
    "2 key properties of an orthogonal matrix:\n",
    "1. All columns are pairwise orthogonal.\n",
    "2. All columns have a magnitude of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal matrix definition:\n",
    "$$Q^TQ = I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extending that to include other equalities:\n",
    "$$Q^TQ = QQ^T = Q^{-1}Q = QQ^{-1} = I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectangular Q\n",
    "- an orthogonal matrix does not need to be square, however, the transpose of a rectangular orthogonal matrix is only a one-sided inverse.\n",
    "- It's possible for a tall matrix to be orthogonal, but its NOT possible for a wide matrix to be orthogonal.\n",
    "- a wide matrix can't be orthogonal because it can't satisfy both properties of an orthogonal matrix (orthogonal columns, each with magnitude of 1)\n",
    "- that said, wide matrices can be \"almost orthogonal\" - they can have interesting properties that almost meet the criteria of an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "- orthogonal matrices seem a bit magical\n",
    "- one might believe they are rare and difficult to construct\n",
    "- Definitely not!  We already know everything needed to make an orthogonal matrix, and will put the pieces to gether in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 Orthogonalization via Gram-Schmidt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's say you have a set of vectors in $\\mathbb{R}^N$ that is independent but not orthogonal.\n",
    "- You can create a set of orthogonal vectors from the original vectors by applying the Gram-Schmidt process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram-Schmidt steps for creating a set of orthonormal vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with $v_1$ and normalize to unit length:\n",
    "$v^*_1 = \\frac{v_1}{|v_1|}$\n",
    "For all remaining vectors in teh set:\n",
    "2. Orthogonalize $v^*_k$ to all previous vectors\n",
    "3. Normalize $v^*_k$ to unit length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this procedure is a set of orthonormal vectors that, when placed as column vectors in a matrix, yield an orthogonal matrix $Q$.\n",
    "\n",
    "*See detailed example on page 382*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "The Gram-Schmidt procedure is numerically unstable, due to round-off errors that propogate forward to each subsequent vector and affect both the normalization and the orthogonalization.  You'll see an example of this in the code challenges.  Computer programs therefore use numerically stable algorithms that achieve the same conceptual result, based on modifications to the standard Gram-Schmidt procedure or alternative methods such as Givens rotations or Gaussian elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.6 QR decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Gram-Schmidt procedure transforms matrix $A$ into orthogonal matrix $Q$.\n",
    "- Unless $A$ is already an orthogonal matrix, $Q$ will be different than $A$, possibly very different.\n",
    "- Thus, information is lost when going from $A \\rightarrow Q$.\n",
    "- Though info is lost, it's possible to recover the information by QR decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = QR$$\n",
    "- The $Q$ here is the same $Q$ that you learned about above; it's the result of Gram-Schmidt orthogonalization (or other comparable but more numerically stable algorithm)\n",
    "- $R$ is like a \"residual\" matrix that contains the information that was orthogonalized out of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To compute $R$, take advantage of the definition of orthogonal matrices:\n",
    "$$Q^TA = Q^TQR$$\n",
    "- and since $Q^TQ=I$\n",
    "$$Q^TA=R$$\n",
    "\n",
    "*see example on page 385*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizes of Q and R, given A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sizes of $Q$ and $R$ depend on the size of $A$, and on a parameter of the implementation.\n",
    "\n",
    "**Square matrix:**\n",
    "  - if $A$ is a squaree matrix, then $Q$ and $R$ are also square matrices, of the same size as $A$\n",
    "  - this is true regardless of the rank of $A$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tall matrix:**\n",
    "- computer algorithms can implement the \"economy QR\" or \"full QR\" decomposition\n",
    "- economy QR:\n",
    "  - $Q$ is the same size as $A$, and $R$ will be NxN.\n",
    "  - however it's possible to create a square $Q$ from a tall $A$.\n",
    "  - that's because the columns of $A$ are in $\\mathbb{R}^M$, so even if $A$ has only N columns, there are M-N more possible columns to create that will be orthogonal to the first M.\n",
    "- full QR:\n",
    "  - will have $Q \\in \\mathbb{R}^{MxM}$ and $R \\in \\mathbb{R}^{MxN}$\n",
    "  - in other words, $Q$ is square and $R$ is the same size as $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wide matrix**\n",
    "- there's no economy QR for wide matrices, because $A$ already has more columns than could form a linearly independent set.\n",
    "- Thus, for a wide matrix $A$, the story is the same as for the full QR decomposition of a tall matrix:\n",
    "  - will have $Q \\in \\mathbb{R}^{MxM}$ and $R \\in \\mathbb{R}^{MxN}$\n",
    "  - in other words, $Q$ is square and $R$ is the same size as $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranks of $Q, R$, and $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Q$ will always have its maximum possible rank (M or N depending on its size), even if $A$ is not full rank\n",
    "  - it may seem surprising that the rank of $Q$ can be higher than the rank of $A$, considering that $Q$ is created from A, but it can.\n",
    "- $R$ will have the same rank as $A$\n",
    "  - because $R$ is created from $Q^TA$, the max possible rank of $R$ will be the rank of $A$, because the rank of $A$ is equal to or less than the rank of $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62398737,  0.7539715 ,  0.18789483],\n",
       "       [-0.74263329, -0.65200527,  0.15279724],\n",
       "       [-0.01933606, -0.05213946, -0.27780652],\n",
       "       [-0.24240809,  0.06080777, -0.92960856]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QR decomposition in Python\n",
    "A = np.random.randn(4,3)\n",
    "Q,R = np.linalg.qr(A)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.52939486, -0.32894288,  0.75203594],\n",
       "       [ 0.        ,  1.85820658, -2.13106638],\n",
       "       [ 0.        ,  0.        ,  1.1208514 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.7 Inverse via QR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we noted previously to avoid having computers compute the inverse unless it's necessary, because of the risk of inaccurate results due to numerical instabilities.\n",
    "- The QR decomposition provides a more stable algorithm to compute the matrix inverse, compared to the MCA algorithm learned in the previous chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the $A=QR$ equation to derive the inverse:\n",
    "$$A=QR$$\n",
    "$$A^{-1} = (QR)^{-1}$$\n",
    "$$A^{-1} = R^{-1}Q^{-1}$$\n",
    "$$A^{-1} = R^{-1}Q^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to compute the explicit inverse of $R$, but if $A$ is a dense matrix, then $R^{-1}$ is easier and more stable to calculate than $A^{-1}$, because nearly half the elements in $R$ are zeros, and matrices with a lot of zeros are easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.8 - 13.9 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do with discussion group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.10 - 13.11 Code challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do with discussion group?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
