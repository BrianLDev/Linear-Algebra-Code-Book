{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: pp. 185 - 206  \n",
    "- exercises: pp. 207 - 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python library imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Six things about matrix rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is a single number associated with that matrix, and is relevant for nearly all applications of linear algebra.  Before learning how to compute the rank, here we'll focus on the concept and interpretatinos of the rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The rank of the matrix is indicated by the letter *r* or by *rank*(A), and is a non-negative integer.  A matrix cannot have a rank of -2 or 4.7.  A rank of 0 is possible, but most matrices have a rank > 0.  In fact, only the zeros matrix can have a rank = 0.\n",
    "\n",
    "2. The max possible rank of an $M x N$ matrix is the smaller of $M$ or $N$\n",
    "\n",
    "3. Rank is a property of the entire matrix; it doesn't make sense to talk about the rank of the columns of the matrix, or the rank of the rows of the matrix.\n",
    "\n",
    "4. The following shows terminology for full-rank matrices, depending on their sizses:\n",
    "- $M$ x $M$ matrix: rank($A$) = $M$ --> \"Full rank\"\n",
    "- $M>N$ matrix: rank($A$) = $N$ --> \"Full column rank\"\n",
    "- $M<N$ matrix: rank($A$) = $M$ --> \"Full row rank\"\n",
    "- if a matrix rank is less than the smaller of M or N, then it can be called any of: \"reduced rank\", \"rank-deficient\", \"degenerate\", \"low-rank\", or \"singular\"\n",
    "\n",
    "5. The rank indicates the number of dimensions of *information* contained in the matrix. This is not the same as the total number of columns or rows in the matrix.\n",
    "\n",
    "6. There are several definitions of rank that you will learn throughout this book, and several algorithms for computing the rank.  However, the key definition to keep in mind is that the **rank of a matrix is the largest number of columns that can form a linearly independent set.**  This is exactly the same as: **the largest number of rows that can form a linearly independent set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection - Why all the full about rank?\n",
    "\n",
    "There are some operations in linear algebra that are valid *only* for full-rank matrices (the matrix inverse being the most important). Other operations are valid on reduced-rank matrices (e.g. eigen-decomposition) but having full rank endows some additional properties.  Furthermore, many computer algorithms returm more reliable results when using full-rank compared to reduced rank matrices.  Indeed, one of the main goals of *regularization* in statistics and machine-learning is to increase numerical stability by ensuring that data matrices are full rank.\n",
    "\n",
    "So yeah, matrix rank is a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Interpretations of matrix rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think of a matrix as comprising a set of vectors, then the rank of the matrix corresponds to the largest number of vectors that can form a linearly independent set.  (Remember that a set of vectors is linearly independent if no vector can be expressed as a linear combination of the other vectors.)\n",
    "\n",
    "i.e. ignore any vectors of a matrix that are simply scaled versions of another vector in the matrix, or scaled combination of 2 or more vectors in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rank is the dimensionality of the sub-space spanned by the columns (or the rows) of the matrix.  This is not necessarily that same as the ambient dimensionality of the space containing the matrix.\n",
    "- For an example, we can think of the matrix  $v = [4 \\ 0 \\ 1]^T$  as a vector that lives in $\\mathbb{R}^3$, but it's subspace only spans 1D.\n",
    "  - This is because we can also reinterpret the 3D vector as 3 points living in $\\mathbb{R}^1$ (3 points on a line), and you can scale each of the 3 points to equal another.\n",
    "- **In fact, all isolated vectors have a rank of 1 (except the zeros vector with rank 0).**\n",
    "- Geometrically, the rank of a matrix is the dimensionality of the subspaces spanned by either the columns or the rows.\n",
    "- Regardless of the perspective you take (a column focused or row focused perspective), the dimensionality of the subspace spanned by those vectors--and thus, the rank of the matrix--is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Computing matrix rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the matrix rank of medium/large matrices is not trivial.  In fact, beyond small matrices, computers cannot actually *compute* the rank of a matrix; they can only *estimate* the rank to a reasonable degree of certainty.  That said, computing the rank of small matrices is not too difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are 3 methods to compute the rank of a matrix.  The first can be implemented with current knowledge, the other 2 will be explained later in the book.\n",
    "\n",
    "1. Count the largetst number of columns (or rows) that can form a linearly independent set.  This involves a bit of trial-and-error and a bit of educated guessing.  You can follow the same tips for determining linear independence from Ch 4 (page 97).\n",
    "\n",
    "2. Count the number of pivots in the echelon or row-reduced echelon form of the matrix (Ch 10).\n",
    "\n",
    "3. Count the number of nonzero singular values from a singular value decomposition of the matrix (Ch 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Compute rank using Python\n",
    "A = np.random.randn(3,6)\n",
    "r = np.linalg.matrix_rank(A)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Rank and scalar multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scalar multiplication has no effect on the rank of a matrix, with one exception when the scalar is 0.\n",
    "- Note: This is clear because rank excludes scaled versions of vectors, so scaling all vectors equally has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n"
     ]
    }
   ],
   "source": [
    "# Show that scaling has no effect on rank\n",
    "scalar = np.random.randn()\n",
    "M = np.random.randn(3, 5)\n",
    "rank = np.linalg.matrix_rank(M)\n",
    "rank_scaled = np.linalg.matrix_rank(scalar * M)\n",
    "print(rank, rank_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Rank of added matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- knowing the ranks of $A$ and $B$ doesn't automatically mean you'll know the rank of $A + B$\n",
    "- But it *does* provide an upper bound on the rank of $A + B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule for max rank of added matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$rank(A + B) \\leq rank(A) + rank(B)$$\n",
    "(plus additional constraint The max possible rank of an $M x N$ matrix is the smaller of $M$ or $N$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.\n",
    "- A = 5x7 matrix\n",
    "- B = 5x7 matrix\n",
    "- if $rank(A)$ = 2, and $rank(B)$ = 5, then the formula says that $rank(A + B) \\leq$ 7.\n",
    "- but then you need to include the additional constraint (see multiple constraints below) where the max rank is smaller of M or N\n",
    "- Thus, the max rank of A + B is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtraction\n",
    "- the addition rules also apply to subtraction, since subtraction is just addition which is multiplied by a scalar of -1, and scalars do not affect rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple constraints\n",
    "\n",
    "- there are multiple constraints on the rank of a matrix. (e.g. the largest possible rank is the smaller of $M$ or $N$)\n",
    "- So to calculate max rank in addition, you need to include the rule/formula above, along with the constraint that rank is smaller of M or N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Rank of multiplied matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As with summed matrices, you cannot know the exact rank of a multiplied matrix product simply from the ranks of $A$ and $B$.\n",
    "- But as with summed matrices, there is a rule for the upper bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule for max rank of multiplied matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$rank(AB) \\leq min\\{rank(A), rank(B)\\}$$\n",
    "\n",
    "i.e. The smaller of $rank(A)$, $rank(B)$ is the largest possible rank of the product matrix\n",
    "\n",
    "e.g. $rank(A)$ = 2, $rank(B)$ = 5, then $rank(A + B) \\leq$ 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to understand this rule?\n",
    "- You can think about it in terms of the column space of the matrix $C = AB$ (which is the subspace spanned by the columns of a matrix).\n",
    "- Think of the $j^{th}$ column of $C$ as being the matrix-vector product of matrix $A$ and the $j^{th}$ column in $B$.\n",
    "$$Ab_j = c_j$$\n",
    "- This means that each column of $C$ is a linear combination fo columns of $A$ with scalar weights defined by the corresponding column in $B$.\n",
    "- In other words, each column of $C$ is in the subspace spanned by the columns of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "- The rules in the previous 2 sections prepare for the next 2 sections, which have **major** implications for applied linear algebra, primarily statistics and machine-learning.\n",
    "- The more comfortable you are with matrix rank, the more intuitive advanced linear concepts will be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Rank of $A, A^T, A^TA, AA^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key take-home message from this section is that these 4 matrices: $A, A^T, A^TA, AA^T$ all have exactly the same rank.\n",
    "- We already know that $A$ and $A^T$ have the same rank because rank can be determined by either column or row perspective, they are the same.  So transposing a matrix has no effect on rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- proving that $A^T$ and $AA^T$ have the same rank is not as easy and requires use of tools not yet leaned in this book.\n",
    "  - The author provided some proofs on pages 197-199.  Bookmark and return to them later if needed to review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Rank of random matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A \"random matrix\" is a matrix that contains elements drawn at random from any type of distribution: normal/Gaussian, uniform, Poisson, etc.\n",
    "- Random matrices have some interesting properties, and there are entire theories built around random matrices.\n",
    "- The most interesting property of random matrices that is relevant for this book is that they are basically always full rank.\n",
    "  - note: this is intuitive since true randomness should have no linear dependency except the extremely rare case of spurious/accidental ones.\n",
    "- As long as the range of possible numbers to select from is large (e.g. any real between 0 and 1) then spurious/accidental linear dependency is extremely rare. But be aware that this no longer applies when the range of possible numbers is narrow (e.g. integers from 1 - 5).\n",
    "- Thus, whenever you create random matrices using computers (using floating point numbers), you can safely assume that their rank is the maximum possible rank.\n",
    "  - this is useful because you can create matrices with any pre-defined rank you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Boosting to full-rank by \"shifting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full-rank square matrices are absolutely fabulous to work with, but many matrices are rank-deficient.  So what are we to do?\n",
    "- One solution is to transform a rank-deficient matrix into a full-rank matrix through \"shifting\" which we learned about in Ch 5.8.\n",
    "  - reminder: \"shifting\" a matrix means to add a multiple of the identity matrix ($A + \\lambda I = \\~A$) which adds a tiny quantity to the diagonal elements without changing the off-diagonal elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we shift by a very small scalar number, then the matrix $\\~A$ approaches the original $A$ matrix.\n",
    "- If we shift by a very large scalar number, then the matrix $\\~A$ approaches the Identity matrix $I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the context of statistics and machine learning, \"shifting\" is also called *regularization* or *matrix smoothing*.\n",
    "- It is an important procedure for multivariate analyses such as principal components analysis and generalized eigendecomposition (which are the mathematical backbones of data compression and linear discriminant analyses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 Difficulties in computing rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author briefly gives a explanation of why it's difficult to compute the rank of large matrices:\n",
    "- Computers suffer from floating point rounding errors that lead to uncertainties in distinguishing very small numbers from true 0.\n",
    "- Numbers < $10^-15$ are considered to be 0 plus a small computer rounding error.\n",
    "- Computer software (e.g. Python or MATLAB) will have some threshold for rounding numbers, and that threshold can affect the rank of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_matrix_rank_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mmatrix_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Return matrix rank of array using SVD method\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Rank of the array is the number of singular values of the array that are\u001b[0m\n",
      "\u001b[0;34m    greater than `tol`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    .. versionchanged:: 1.14\u001b[0m\n",
      "\u001b[0;34m       Can now operate on stacks of matrices\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    M : {(M,), (..., M, N)} array_like\u001b[0m\n",
      "\u001b[0;34m        Input vector or stack of matrices.\u001b[0m\n",
      "\u001b[0;34m    tol : (...) array_like, float, optional\u001b[0m\n",
      "\u001b[0;34m        Threshold below which SVD values are considered zero. If `tol` is\u001b[0m\n",
      "\u001b[0;34m        None, and ``S`` is an array with singular values for `M`, and\u001b[0m\n",
      "\u001b[0;34m        ``eps`` is the epsilon value for datatype of ``S``, then `tol` is\u001b[0m\n",
      "\u001b[0;34m        set to ``S.max() * max(M.shape) * eps``.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 1.14\u001b[0m\n",
      "\u001b[0;34m           Broadcasted against the stack of matrices\u001b[0m\n",
      "\u001b[0;34m    hermitian : bool, optional\u001b[0m\n",
      "\u001b[0;34m        If True, `M` is assumed to be Hermitian (symmetric if real-valued),\u001b[0m\n",
      "\u001b[0;34m        enabling a more efficient method for finding singular values.\u001b[0m\n",
      "\u001b[0;34m        Defaults to False.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.14\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns\u001b[0m\n",
      "\u001b[0;34m    -------\u001b[0m\n",
      "\u001b[0;34m    rank : (...) array_like\u001b[0m\n",
      "\u001b[0;34m        Rank of M.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Notes\u001b[0m\n",
      "\u001b[0;34m    -----\u001b[0m\n",
      "\u001b[0;34m    The default threshold to detect rank deficiency is a test on the magnitude\u001b[0m\n",
      "\u001b[0;34m    of the singular values of `M`.  By default, we identify singular values less\u001b[0m\n",
      "\u001b[0;34m    than ``S.max() * max(M.shape) * eps`` as indicating rank deficiency (with\u001b[0m\n",
      "\u001b[0;34m    the symbols defined above). This is the algorithm MATLAB uses [1].  It also\u001b[0m\n",
      "\u001b[0;34m    appears in *Numerical recipes* in the discussion of SVD solutions for linear\u001b[0m\n",
      "\u001b[0;34m    least squares [2].\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This default threshold is designed to detect rank deficiency accounting for\u001b[0m\n",
      "\u001b[0;34m    the numerical errors of the SVD computation.  Imagine that there is a column\u001b[0m\n",
      "\u001b[0;34m    in `M` that is an exact (in floating point) linear combination of other\u001b[0m\n",
      "\u001b[0;34m    columns in `M`. Computing the SVD on `M` will not produce a singular value\u001b[0m\n",
      "\u001b[0;34m    exactly equal to 0 in general: any difference of the smallest SVD value from\u001b[0m\n",
      "\u001b[0;34m    0 will be caused by numerical imprecision in the calculation of the SVD.\u001b[0m\n",
      "\u001b[0;34m    Our threshold for small SVD values takes this numerical imprecision into\u001b[0m\n",
      "\u001b[0;34m    account, and the default threshold will detect such numerical rank\u001b[0m\n",
      "\u001b[0;34m    deficiency.  The threshold may declare a matrix `M` rank deficient even if\u001b[0m\n",
      "\u001b[0;34m    the linear combination of some columns of `M` is not exactly equal to\u001b[0m\n",
      "\u001b[0;34m    another column of `M` but only numerically very close to another column of\u001b[0m\n",
      "\u001b[0;34m    `M`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    We chose our default threshold because it is in wide use.  Other thresholds\u001b[0m\n",
      "\u001b[0;34m    are possible.  For example, elsewhere in the 2007 edition of *Numerical\u001b[0m\n",
      "\u001b[0;34m    recipes* there is an alternative threshold of ``S.max() *\u001b[0m\n",
      "\u001b[0;34m    np.finfo(M.dtype).eps / 2. * np.sqrt(m + n + 1.)``. The authors describe\u001b[0m\n",
      "\u001b[0;34m    this threshold as being based on \"expected roundoff error\" (p 71).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The thresholds above deal with floating point roundoff error in the\u001b[0m\n",
      "\u001b[0;34m    calculation of the SVD.  However, you may have more information about the\u001b[0m\n",
      "\u001b[0;34m    sources of error in `M` that would make you consider other tolerance values\u001b[0m\n",
      "\u001b[0;34m    to detect *effective* rank deficiency.  The most useful measure of the\u001b[0m\n",
      "\u001b[0;34m    tolerance depends on the operations you intend to use on your matrix.  For\u001b[0m\n",
      "\u001b[0;34m    example, if your data come from uncertain measurements with uncertainties\u001b[0m\n",
      "\u001b[0;34m    greater than floating point epsilon, choosing a tolerance near that\u001b[0m\n",
      "\u001b[0;34m    uncertainty may be preferable.  The tolerance may be absolute if the\u001b[0m\n",
      "\u001b[0;34m    uncertainties are absolute rather than relative.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    References\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    .. [1] MATLAB reference documention, \"Rank\"\u001b[0m\n",
      "\u001b[0;34m           https://www.mathworks.com/help/techdoc/ref/rank.html\u001b[0m\n",
      "\u001b[0;34m    .. [2] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,\u001b[0m\n",
      "\u001b[0;34m           \"Numerical Recipes (3rd edition)\", Cambridge University Press, 2007,\u001b[0m\n",
      "\u001b[0;34m           page 795.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    >>> from numpy.linalg import matrix_rank\u001b[0m\n",
      "\u001b[0;34m    >>> matrix_rank(np.eye(4)) # Full rank matrix\u001b[0m\n",
      "\u001b[0;34m    4\u001b[0m\n",
      "\u001b[0;34m    >>> I=np.eye(4); I[-1,-1] = 0. # rank deficient matrix\u001b[0m\n",
      "\u001b[0;34m    >>> matrix_rank(I)\u001b[0m\n",
      "\u001b[0;34m    3\u001b[0m\n",
      "\u001b[0;34m    >>> matrix_rank(np.ones((4,))) # 1 dimension - rank 1 unless all 0\u001b[0m\n",
      "\u001b[0;34m    1\u001b[0m\n",
      "\u001b[0;34m    >>> matrix_rank(np.zeros((4,)))\u001b[0m\n",
      "\u001b[0;34m    0\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_uv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "## inspect the source code used to compute rank to check rounding threshold\n",
    "??np.linalg.matrix_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about the difficulty in computing rank geometrically.\n",
    "- Imagine a 3 x 3 matrix that represents some data collected from a satellite.\n",
    "  - The columns are in $\\mathbb{R}^3$ and let's imagine we know for a fact the 3 vectors all lie on a 2D plane (based on satellite sensor design).\n",
    "  - this means, we know for a fact that the rank is 2.\n",
    "- The satellite data is imperfect and there is a tiny bit of noise.\n",
    "  - this noise causes the vectors to point slightly off of the 2D plane\n",
    "- Due to the imperfect data, a computer will calculate the rank of this matrix as 3, but it really should be rank 2 excluding the noise.\n",
    "- So given this info, you might want your rank-estimating-algorithm to ignore some small amount of noise based on what you know about the real life data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Rank and span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Put the vectors from set *S* into a matrix $S$.\n",
    "2. Compute the rank of $S$. Call that rank $r1$.\n",
    "3. Augment $S$ by $v$, thus creating $S_v = S \\lfloor\\rfloor v$.\n",
    "4. Compute the rank of $S_v$. Call that rank $r2$.\n",
    "5. If $r2 > r1$ then $v$ is **not** in the span of S.  \n",
    "If $r2 = r1$ then $v$ **is** in the span of S.  \n",
    "If $r2 <> r1$ then check your math or code for a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12 - 7.13 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do some for group discussion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.14 - 7.15 Code Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The goal of this code challenge is to create random matrices with any arbitrary rank (though still limited by the constraints presented in this chapter). In particular, combine standard matrix multiplication (previous chapter) with the rule about rank and matrix multiplication (Equation 7.8) to create reduced-rank matrices comprising random numbers (hint: think about the \"inner dimensions\" of matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Goal rank = 2\n",
    "A = np.random.randn(13,4)\n",
    "B = np.random.randn(4,2)\n",
    "C = A@B\n",
    "rank = np.linalg.matrix_rank(C)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The goal of this code challenge is to explore the tolerance level of your computer for computing the rank of matrices with tiny values.  Start by creating the 5x5 zeros matrix and confirm that its rank is 0.  Then add a 5x5 random numbers matrix scaled by machine-epsilon, which is the computer's estimate of its numerical precision due to round-off errors.  Now the rank of that summed matrix will be 5. Finally, keep scaling down the machine-epsilon until the rank of the summed matrix is 0. You can also compute the Frobenius norm to get a sense of the magnitude of the values in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.zeros((5,5))\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.matrix_rank(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(5,5)\n",
    "m_epsilon = np.finfo(float).eps\n",
    "print(m_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "A_scaled = A * m_epsilon\n",
    "C = Z + A_scaled\n",
    "print(np.linalg.matrix_rank(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.994687149941428e-31\n"
     ]
    }
   ],
   "source": [
    "frob = np.trace(C.T@C)\n",
    "print(frob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm after 1 loops: 3.941685088788491e-62\n",
      "Frobenius norm after 2 loops: 1.9434007920236585e-93\n",
      "Frobenius norm after 3 loops: 9.58170567501884e-125\n",
      "Frobenius norm after 4 loops: 4.724145632722918e-156\n",
      "Frobenius norm after 5 loops: 2.3291836251410566e-187\n",
      "Frobenius norm after 6 loops: 1.1483761893467074e-218\n",
      "Frobenius norm after 7 loops: 5.661931751639373e-250\n",
      "Frobenius norm after 8 loops: 2.79154787931114e-281\n",
      "Frobenius norm after 9 loops: 1.376339366904e-312\n",
      "Frobenius norm after 10 loops: 0.0\n",
      "Frobenius norm after 11 loops: 0.0\n",
      "Frobenius norm after 12 loops: 0.0\n",
      "Frobenius norm after 13 loops: 0.0\n",
      "Frobenius norm after 14 loops: 0.0\n",
      "Frobenius norm after 15 loops: 0.0\n",
      "Frobenius norm after 16 loops: 0.0\n",
      "Frobenius norm after 17 loops: 0.0\n",
      "Frobenius norm after 18 loops: 0.0\n",
      "Frobenius norm after 19 loops: 0.0\n",
      "Frobenius norm after 20 loops: 0.0\n",
      "Reached rank 0 after multiplying by machine epsilon 20 times.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "  C = C * m_epsilon\n",
    "  frob = np.trace(C.T@C)\n",
    "  print(\"Frobenius norm after \" + str(i) + \" loops: \" + str(frob))\n",
    "  rank = np.linalg.matrix_rank(C)\n",
    "  if (rank == 0):\n",
    "    print(\"Reached rank 0 after multiplying by machine epsilon \" + str(i) + \" times.\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
