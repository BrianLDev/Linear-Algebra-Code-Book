{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: pp. 421 - 462\n",
    "- exercises: pp. 463 - 470"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended supplementary videos:\n",
    "- [Eigenvalues and Eigenvectors, Imaginary and Real](https://youtu.be/8F0gdO643Tc) - Physics by Eugene K\n",
    "- [Eigenvectors and eigenvalues | Chapter 14, Essence of linear algebra](https://youtu.be/PFDu9oVAE-g) - 3 Blue 1 Brown\n",
    "- [A quick trick for computing eigenvalues | Chapter 15, Essence of linear algebra](https://youtu.be/e50Bj7jn9IQ) - 3 Blue 1 Brown\n",
    "- [21. Eigenvalues and Eigenvectors](https://youtu.be/cdZnhQjJu4I) - Gilbert Strang (MIT Open Courseware)\n",
    "- [22. Diagonalization and Powers of A](https://youtu.be/13r9QY6cmjc) - Gilbert Strang (MIT Open Courseware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 15.1 What are eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are myriad explanations of eigenvectors and eigenvalues, and most students find most explanations incoherent on first impression.\n",
    "- In this section, Cohen provides 3 explanations that he hopes will build intuition, with additional insights to come later.\n",
    "- notation:  typically eigenvalues are labeled $\\lambda$ and eigenvectors are labeled $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key properties of eigendecomposition:\n",
    "1. Eigendecomposition is defined only for square matrices.  They can be singular or inverible, symmetric or triangle or diagonal; but eigendecomposition can only be performed on square matrices.\n",
    "2. The purpose of eigendecomposition is to extract two sets of features from a matrix: eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an MxM matrix has M eigenvalues and M eigenvectors.\n",
    "- The eigenvalues and eigenvectors are paired 1 to 1.\n",
    "- Importantly, eigenvectors/values are not special properties of the vector alone, nor of the matrix alone.  They are a combination of a particular matrix, a particular vector, and a particular scalar.\n",
    "  - changing any of these qualities (even a single element) will likely destroy this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue equation\n",
    "$$Av = \\lambda v$$\n",
    "- this equation is saying that the effect of multiplying the matrix by the vector is the same as scaling the vector\n",
    "- *note that you cannot divide both sides by $v$, because vector division is undefined.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way to think about matrix-vector multiplicaiton is that matrices act as input-output transformers.\n",
    "- Vector $w$ goes in, and vector $Aw=y$ comes out.\n",
    "- The majority of the time, the resulting vector $y$ will point in a different direction from $w$.\n",
    "- in other words, $A$ rotates the vector.\n",
    "- eigenvectors are the unique case where matrix transformation **does not** rotate the matrix (only scales it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we plot data and draw a trendline, it turns out that line is an eigenvector of the data matrix times its transpose, which is also called a covariance matrix.\n",
    "- these lines can be called the \"principal components\" of a matrix\n",
    "- Principal Components Analysis (PCA) is one of the most important tools in data science (e.g. unsupervised machine learning), and it is nothing more than an eigendecomposition of a data matrix.\n",
    "  - more on PCA in chapter 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubik's cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- think of a Rubuk's cube as a matrix (technically it's a tensor but just go with it).\n",
    "- the information in the cube is scattered around, likewise, patterns of info in a data matrix are often distrubuted across rows and columns.\n",
    "- To solve the cube, you perform rotations on the rows and columns\n",
    "- This specific sequence of rotations is like the eigenvectors: they provide a set of instructions for how to rotate the info in the matrix.\n",
    "- Once you apply all the rotations, the info in the matrix becomes \"ordered\" with all of the similar info packed into one eigenvalue.  Thus, the eigenvalue is analogous to a color.\n",
    "- The completed Rubik's cube is analogous to a procedure called \"diagonalization\" which means to put all of the eigenvectors into a matrix, and all of the eigenvalues into a diagonal matrix.  That diagonal matrix is like the solved Rubik's cube.\n",
    "- *(if the Rubik's cube analogy isn't helpful, just ignore it and use the previous interpretations)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Finding eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigenvectors are like secret passages that are hidden inside the matrix.  To find those secret passages, we need to find the secret keys.  Eigenvalues are those keys.\n",
    "- Thus, eigendecomposition requires first finding the eigenvalues, then using those eigenvalues as \"magic keys\" to unlock the eigenvectors.\n",
    "- To find the eigenvalues of a matrix is to re-write the Eigenvalue equation so that we have some expression equal to the zeros vector.\n",
    "$$Av - \\lambda v = 0$$\n",
    "- since $v$ is a shared component, we can factor it out, but we need to insert the identity matrix after $\\lambda$\n",
    "$$Av - \\lambda I v = 0$$\n",
    "$$(A - \\lambda I) v = 0$$\n",
    "- this equation is familiar: it is the same as the definition of the null space from 8.6.\n",
    "- Thus, we've discovered that when shifting a matrix by an eigenvalue, the eigenvector is in its null space.\n",
    "- That becomes the mechanism for finding the eigenvector, but it's all very theoretical at this point--we still don't konw how to find $\\lambda$!\n",
    "- The key here is to remember what we know about a matrix with a non-trivial null space, in particular, about its rank:\n",
    "  - we know that any square matrix with a non-trivial null space is reduced rank.\n",
    "  - we konw that a reduced rank matrix has a determinant of zero.\n",
    "- this leads to the equation for finding the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation for finding eigenvalues (15.5)\n",
    "$$det(A - \\lambda I) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In 11.3 we learned that the determinant of a matrix is computed by solving the characteristic polynomial, and we saw examples of how a known determinant can allow us to solve for some unknown variable inside the matrix.\n",
    "- That's the situation we have here:\n",
    "  - we have a matrix with 1 missing parameter ($\\lambda$) and we know that its determinant is zero.\n",
    "- and that's how you find the eigenvalues of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues of a 2x2 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for a 2x2 matrix, the characteristic polynomial is a quadratic equation.\n",
    "\n",
    "$$det(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}) = 0$$\n",
    "$$det(\\begin{bmatrix} a - \\lambda & b \\\\ c & d - \\lambda \\end{bmatrix}) = 0$$\n",
    "$$(a - \\lambda)(d - \\lambda) - bc = 0$$\n",
    "$$\\lambda^2 - (a + d)\\lambda + (ad - bc) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- since this is a 2nd degree algebraic equation, there are two $\\lambda$ solutions.\n",
    "- The solutions can be found with the quadratic equation (refresher):\n",
    "$$\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n",
    "\n",
    "*see page 430 for example of finding eigenvalues on 2x2 matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slight shortcut for eigenvalues of 2x2 matrix\n",
    "$$\\lambda - tr(A)\\lambda + det(A) = 0$$\n",
    "\n",
    "*(you still need to solve for $\\lambda$ so it isn't the best shortcut, ub it will get you to the characteristic polynimial slighly faster)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues of a 3x3 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The algebra gets more complicated, but the principle is the same: shift the matrix by $-\\lambda$ and solve for $\\Delta = 0$\n",
    "- the characteristic polynomial produces a 3rd order equation here, so there will be 3 eigenvalues as roots for the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$det(\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}) = 0$$\n",
    "$$det(\\begin{bmatrix} a-\\lambda & b & c \\\\ d & e-\\lambda & f \\\\ g & h & i-\\lambda \\end{bmatrix}) = 0$$\n",
    "$$(a - \\lambda)(e - \\lambda)(i - \\lambda) + bfg + cdh - c(e - \\lambda)g - bd(i - \\lambda) - (a - \\lambda)fh = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M columns, M $\\lambda$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- per the fundamental theorem of algebra, any m-degree polynomial has m solutions.\n",
    "- thus, an MxM matrix has an Mth order polynomial, which has M roots, and M eigenvalues.\n",
    "- **an MxM matrix has M eigenvalues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "- eigenvalues have no *intrinsic* sorting.\n",
    "- we can come up with *sensible* sorting.\n",
    "  - e.g. ordering eigenvalues according to their position on the number line or magnitude (distance from zero)\n",
    "  - or by a property of their corresponding eigenvectors.\n",
    "- Sorted eigenvalues can facilitate data analyses, but eigenvalues are an intrinsically unsorted set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Finding eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The eigenvectors of a matrix reveal important \"directions\" in that matrix.\n",
    "- you can think of those directions as being invariant to rotations.\n",
    "- The eigenvectors are encrypted inside the matrix, and each eigenvalue is the decryption key for each eigenvector.\n",
    "- Once you have the key, put it in the matrix, turn it, and the eigenvector will be revealed.\n",
    "- In particular, once you've identified the eigenvalues, shift the matrix by each $-\\lambda_i$ and find a vector $v_i$ in theh null space of that shifted matrix.\n",
    "  - this is the eigenvector associated with eigenvalue $\\lambda_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equations for finding eigenvectors**\n",
    "\n",
    "two methods:\n",
    "$$(A - \\lambda_i I)v_i = 0$$\n",
    "$$v_i(A - \\lambda_i I), \\;\\;\\;\\;\\; v_i \\in N$$\n",
    "\n",
    "*see p. 433 for examples of finding eigenvectors in 2x2 matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interestingly, although eigenVALUES are unique, there are an infinite number of eigenVECTORS that come out of the equation (all scaled versions of the same vector of course)\n",
    "- Thus, **the true interpretation of an eigenvector is a basis vector for a 1D subspace**\n",
    "  - i.e. the \"preferred\" eigenvector is the unit-length basis vector for the null space of the matrix shifted by its eigenvalue.\n",
    "  - computer software like MATLAB and Python libraries will provide the unit length eigenvector\n",
    "  - that said, it can be hard/messy to calculate by hand, so typically when solving problems by hand its easier to use integer elements.\n",
    "\n",
    "*see p. 435 for example of finding eigenvectors in 3x3 matrix*\n",
    "- the process is essentially the same as 2x2 matrix, only a bit more challenging since you are dealing with a 3rd order equation and the determinant calc for a 3x3 matrix is slightly more involved.\n",
    "  - i.e. the process itself is relatively easy to understand but the work can be tedious.\n",
    "- so do a few by hand to internalize the process, then in the future you can let the computer do the work for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 Diagonalization via eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that we have eigenvalues and eigenvectors, we can step back and look at the big picture.\n",
    "- each eigen-equation can separately be listed as:\n",
    "$$Av_1 = \\lambda_1 v_1$$\n",
    "$$Av_2 = \\lambda_2 v_2$$\n",
    "$$...$$\n",
    "$$Av_m = \\lambda_m v_m$$\n",
    "- This is clunky and ugly, but fortunately we can clean it up and make it compact by doing the following:\n",
    "  - Have each eigenvector be a column in a matrix.\n",
    "  - Have each eigenvalue be an element in a diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an equation this is represented as:\n",
    "\n",
    "$$AV = V\\Lambda$$\n",
    "\n",
    "Notes about the equation:\n",
    "- $V$ is the matrix containing the eigenvectors, each column being a separate eigenvector\n",
    "- $\\Lambda$ is the diagonal matrix containing all eigenvalues\n",
    "  - the $\\Lambda$ symbol is the capitalized version of $\\lambda$ in Greek\n",
    "- $\\Lambda$ needs to post-multiply $V$ on the right hand side of the equation ($V \\Lambda$).  Pre-multiply ($\\Lambda V$)will not work.\n",
    "  - explanation as to why can be found on p. 436-437\n",
    "  - 1) we want the eigenvalues to scale each column, not each row.\n",
    "  - 2) if the equation read $AV = \\Lambda V$, then we could multiply both sides by $V^{-1}$, resulting in $A=\\Lambda$ which is typically not true.\n",
    "- remember that each eigenvector must be matched pair-wise with its corresponding eigenvalue\n",
    "  - i.e. if you re-arrange one, make sure you re-arrange the other to maintain pair-wise matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The equation $AV = V\\Lambda$ is not only a practical short-hand for a set of equations; it provides an important conceptual insight into one of the core ideas in eigendecomposition:\n",
    "  - Finding a set of basis vectors such that the matrix is diagonal in that basis space.\n",
    "- This can be seen by left-multiplying both sides of the equation by $V^{-1}$\n",
    "  - (which is a valid operation if we assume that all eigenvectors form an independent set. This happens when there are M distinct eigenvalues)\n",
    "$$V^{-1}AV = \\Lambda$$\n",
    "- thus, matrix $A$ is diagonal in basis $V$.\n",
    "- That's why eigendecomposition is also sometimes called *diagonalization*.\n",
    "- To diagonalize matrix $A$ means to find some matrix of basis vectors such that $A$ is a diagonal matrix in that basis space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Right-multiplying by $V^{-1}$ reveals that a diagonal matrix is \"hidden\" inside each diagonalizable square matrix; the rotation matrix $V$ reveals that hidden organization:\n",
    "$$A = V \\Lambda V^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's revisit the Rubik's cube analogy from earlier in the chapter...\n",
    "- in the equation $V^{-1}AV = \\Lambda$:\n",
    "  - $A$ is the scrambled Rubik's cube with all sides having inter-mixed colors\n",
    "  - $V$ is the set of rotations that you apply to the Rubik's cube in order to solve the puzzle\n",
    "  - $\\Lambda$ is the cube in its \"ordered\" form with each side having exactly one color\n",
    "  - $V^{-1}$ is the inverse of the rotations, which is how you would get from the ordered form to the original mixed form.\n",
    "\n",
    "*see figure 15.5 on p. 438 for a visual example of Diagonalization of a matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "The previous reflection box mentioned sorting eigenvalues, and figure 15.5 shows eigenvalues sorted ascending along the diagonals.  Re-sorting eigenvalues is fine, but you need to be diligent to apply the same re-sorting to the columns of $V$, otherwise the eigenvealues and their associated eigenvectors will be mismatched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  0.],\n",
       "       [ 0., -1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain eigenvalues and eigenvectors in Python\n",
    "import numpy as np\n",
    "A = np.array([[2, 3], [3, 2]])\n",
    "L, V = np.linalg.eig(A)   # outputs a vector for the eigenvalues (L) and a matrix for the eigenvectors (V)\n",
    "L = np.diag(L)            # put eigenvalues in a diagonal matrix\n",
    "# L = np.eye(len(A)) * L    # alternate method to put eigenvalues in a diagonal matrix\n",
    "L   # print the eigenvalue diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678, -0.70710678],\n",
       "       [ 0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V   # print the eigenvectors (unit length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5 Conditions for diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not all square matrices are diagonalizable, only matrices where the equation $A = V\\Lambda V^{-1}$ is true/valid are diagonalizable.\n",
    "- Fortunately, for applied linear algebra, \"most\" matrices are diagonalizable\n",
    "  - i.e. the square matrices that show up in statistics, machine learning, data science, computational simulations, and olter simulations are likely diagonalizable.\n",
    "- **Importantly, all symmetric matrices are diagonalizable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are matrices for which no matrix $V$ can make that decomposition equation true (non-diagonalizable)\n",
    "- Here's an example of a non-diagonalizable matrix:\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "  1 & 1 \\\\\n",
    "  -1 & -1\n",
    "\\end{bmatrix}\n",
    ", \\;\\;\\;\\;\\;\n",
    "\\lambda = \\{0, 0\\}\n",
    ", \\;\\;\\;\\;\\;\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    "  1 & -1 \\\\\n",
    "  -1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- notice that the matrix is rank-1 and yet has two zero-valued eigenvalues.  This means that our diagonal matrix of eigenvalues would be the zeros matrix, and it is impossible to reconstruct the original matrix using $\\Lambda=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilpotent matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an entire category of matrices that is non-diagonalizable, called *nilpotent* matrices.\n",
    "- A nilpotent matrix means that for some matrix power $k$, $A^k=0$\n",
    "  - i.e. keep multiplying the matrix by itself and eventually you'll get the zeros matrix\n",
    "- below is an example of a rank-1 nilpotent matrix with k=2\n",
    "  - confirm for yourself that $AA = 0$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- all triangle matrices that have zeros on the diagonal are nilpotent, and all have zero-valued eigenvalues, and thus cannot be diagonalized.\n",
    "- all hope is not lost, because the singular value decomposition (SVD) is valid on all matrices, even the non-diagonalizable ones.\n",
    "  - SVD is covered in ch: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6 Distinct vs. repeated eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many square matrices have M distinct eigenvalues, which is nice because:\n",
    "- **distinct eigenvalues always lead to distinct (linearly independent) eigenvectors**\n",
    "\n",
    "*see p. 441 - 442 for proof that distinct eigenvalues always lead to distinct eigenvectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Repeated eigenvalues complicate matters because they sometimes have distinct eigenvectors and sometimes not.\n",
    "\n",
    "*see examples on p. 443-444*\n",
    "\n",
    "- now that we've seen examples of the possible outcomes of repeated eigenvalues, the possible outcomes are:\n",
    "  - only one eigenvector or distinct eigenvectors\n",
    "  - an infinity of possible sets of distinct eigenvectors\n",
    "- which of these possibilities depends on the numbers in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric interpretation of repeated eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated eigenvalues can lead to one of two situations:\n",
    "1. both eigenvectors can lie on the same 1D subspace.  In that case, the eigenspace won't span the entire ambient space $\\mathbb{R}^M$; it will be a smaller-dimensional subspace.\n",
    "2. There are two distinct eigenvectors associated with one eigenvalue.  In this case, there isn't a unique eigen*vector*; instead, there is a unique eigen*plane* and the two eigenvectors are basis vectors for that eigenplane.  Any two independent vectors in the plane can be used as a basis.  It is convenient to define those vectors to be orthogonal, and this is what computer programs will return.\n",
    "\n",
    "*see figure 15.6 on p. 446 for a visual of both situations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Do you really need to worry about repeated eigenvalues?  In the author's personal experience, he finds nearly identical eigenvalues to be infrequent, but common enough that one needs to keep an eye out for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.7 Complex eigenvalues or eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $4ac > b^2$ in the quadratic equation, then you will end up with a square root of a negative number, which means the eigenvalues will be complex numbers.  And complex eigenvalues lead to complex eigenvectors.\n",
    "- Don't be afraid of complex numbers or complex solutions; they are perfectly natural and can arise even from matrices that contain all real values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical example of complex eigenvalues is the identity matrix with one row-swap and a minus sign:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow \\Lambda = \n",
    "\\begin{bmatrix}\n",
    "i & 0 \\\\\n",
    "0 & -i\n",
    "\\end{bmatrix}\n",
    ",\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "-i & i\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow \\Lambda = \n",
    "\\begin{bmatrix}\n",
    "i & 0 & 0 \\\\\n",
    "0 & -i & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    ",\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 \\\\\n",
    "-1 & -1 & 0 \\\\\n",
    "-i & i & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complex solutions can also arise from \"normal\" matrices with real-valued entries.  \n",
    "- Example:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & 15 \\\\\n",
    "-6 & 4\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow \\Lambda = \n",
    "\\begin{bmatrix}\n",
    "1.5+9.2i & 0 \\\\\n",
    "0 & 1.5-9.2i\n",
    "\\end{bmatrix}\n",
    ",\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    ".85 & .85 \\\\\n",
    ".1+.5i & .1-.5i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- It is no coincidence that the two solutions is a pair of complex conjugates.\n",
    "  - For a 2x2 matrix, complex conjugate pair solutions are immediately obvious from equation 15.7 (p. 428)\n",
    "  - A complex number can only come from the square root in the numerator, which is preceeded by a $\\pm$ sign.\n",
    "  - Thus, the two solutions will have the same real part and flipped-sign imaginary part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This generalizes to larger matrices:\n",
    "  - A real-valued matrix with complex eigenvalues has solutions that come in pairs: $\\lambda$ and $\\bar{\\lambda}$\n",
    "  - furthermore, their associated eigenvectors also come in conjugate pairs $v$ and $\\bar{v}$\n",
    "\n",
    "$$Av = \\lambda v$$\n",
    "$$\\overline{Av} = \\overline{\\lambda v}$$\n",
    "\n",
    "- Complex-valued solutions in eigendecomposition can be difficult to work with in applications with real datasets, but there is nothing in principle weird or strange about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.8 Eigendecomposition of a symmetric matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Everyone who works with matrices has a special place in their heart for the elation that comes with symmetry across the diagonal.\n",
    "- In this section, we'll lear 2 additional properties that make symmetric matrices really great to work with.\n",
    "- Eigendecomposition of a symmetric matrix has two notable features:\n",
    "  - orthogonal eigenvectors (assuming distinct eigenvalues; for repeated eigenvalues the eigenvectors can be crafted to be orthogonal)\n",
    "  - real-valued solutions (as opposed to complex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the matrix is symmetric, then all of its eigenvectors are pairwise orthogonal.\n",
    "\n",
    "*see p. 449 for proof of this statement*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crux of the proof:\n",
    "$$(\\lambda_1 - \\lambda_2)v^T_1 v_2$$\n",
    "\n",
    "- this equation says that two quantities multiply to produce 0, which means that one or both of those quantities must be zero.\n",
    "  - $(\\lambda_ - \\lambda_2)$ cannot equal zero because we began from the assumption that they are distinct.\n",
    "  - Therefore, $v^T_1 v_2$ must be zero, which means that $v_1 \\perp v_2$ (the 2 eigenvectors are orthogonal)\n",
    "- *note that this proof is only valid for symmetric matrices, when $A^T = A$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Orthogonal eigenvectors are a big deal.  It means that the dot product between any two non-identical columns will be zero:\n",
    "\n",
    "$$v^T_i v_j = \\Biggl\\{ \\begin{matrix} ||v^2|| \\;\\;\\; \\text{if } i = j \\\\ 0 \\;\\;\\;\\;\\;\\;\\;\\; \\text{if } i \\neq j\\end{matrix}$$\n",
    "\n",
    "- when putting all of those eigenvectors as columns into a matrix $V$, then $V^TV$ is a diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that eigenvectors are important because of their direction, not magnitude.  And remember that it's convenient to have unit-length eigenvectors.\n",
    "- So we can rewrite the above equation using unit-norm eigenvectors:\n",
    "\n",
    "$$v^T_i v_j = \\Biggl\\{ \\begin{matrix} 1 \\;\\;\\; \\text{if } i = j \\\\ 0 \\;\\;\\; \\text{if } i \\neq j\\end{matrix}$$\n",
    "\n",
    "- hopefully this looks familiar, because it is the definition of an orthogonal matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and this means:\n",
    "$$V^TV = I$$\n",
    "$$V^T = V^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thus, the eigenvectors of a symmetric matrix form an orthogonal matrix.\n",
    "- This is an important property with implications for statistics, multivariate signal processing, data compression, and other applications.\n",
    "  - more coming in Ch 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-valued solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's examine the property that real-valued symmetric matrices always have real-valued eigenvalues (and therefore also real-valued eigenvectors).\n",
    "- 6 steps to the proof:\n",
    "\n",
    "$$ 1) Av = \\lambda v$$\n",
    "$$ 2) (Av)^H = (\\lambda v)^H$$\n",
    "$$ 3) v^H A = \\lambda^H v^H$$\n",
    "$$ 4) v^H Av = \\lambda^Hv^Hv$$\n",
    "$$ 5) \\lambda v^Hv = \\lambda^H v^Hv$$\n",
    "$$ 6) \\lambda = \\lambda^H$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "1. the basic eigenvalue equation\n",
    "2. we take the Hermitian of both sides of the equation\n",
    "3. the Hermitian is implemented.  Because $A$ is symmetric and comprises real numbers, $A^H = A^T = A$\n",
    "4. both sides of the equation are right-multiplied by the eigenvector $v$\n",
    "5. $Av$ is turned into its equivalent $\\lambda v$.  $V^Hv$ is the magnitude squared of vector $v$ and can simply be divided away (remember that $v \\neq 0$).\n",
    "6. this brings us to the conclusion that $\\lambda = \\lambda^H$.  i.e. a number is equal to its complex conjugate only when b=0, which means it is a real number.\n",
    "\n",
    "*note - if the matrix is not symmetric, we cannot proceed to step 3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- final note: some people use different letters to indicate the eigendecomposition of a symmetric matrix.\n",
    "- you might see the following options:\n",
    "\n",
    "$$A = PDP^{-1}$$\n",
    "$$A = UDU^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9 Eigenvalues of singular matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every singular matrix has at least one zero-valued eigenvalue.\n",
    "- And every full-rank matrix has no zero-valued eigenvalues.\n",
    "\n",
    "*see examples on p. 452-453 demonstrating that eigenvectors associated with zero-valued eigenvalues are not unusual*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the number of non-zero eigenvalues equals the rank for some matrices, but this is not generally true for all matrices.\n",
    "- there are several explanations for why singular matrices have at least one zero-valued eigenvalue\n",
    "1. the determinant of a matrix equals the product of the eigenvalues, and the determinant of a singular matrix is 0, so at least one eigenvalue must be zero.\n",
    "2. Reconsidering equation 15.4: $(A-\\lambda I) v = 0$, if we assume that $\\lambda = 0$, then we're not shifting the matrix and can rewrite the equation as $Av=0$.  Because the zeros vector is not an eigenvector, matrix $A$ already has a non-trivial vector in its null space, hence it is singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.10 Eigenlayers of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section will tie together eigendecomposition with the 'layer perspective' of matrix multiplication.\n",
    "- it will also set us up to understand the \"spectral theory of matrices\" and applications such as data compression and PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- consider computing the outer product of one eigenvector with itself: \n",
    "  - that will produce an MxM rank-1 matrix.\n",
    "  - the norm of the matrix will also be 1, because it is formed from a unit-norm vector\n",
    "- An MxM matrix has M eigenvectors, and thus, M outer product matrices can be formed from the set of eigenvctors\n",
    "- What would happen if we sum together all of those outer product matrices?\n",
    "  - not much, but the sum would not equal the original matrix $A$.\n",
    "  - Why not?  Because eigenvectors have no intrinsic length, they need the eigenvalues to scale them.\n",
    "  - therefore, we'll multiply each eigenvector outer product matrix by its corresponding eigenvalue.\n",
    "- Now we're in an interesting situation, because this sum will exactly reproduce the original matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in other words, we can reconstruct the matrix one \"eigenlayer\" at a time:\n",
    "$$A = \\sum_{i=1}^{M} v_i \\lambda_i v^T_i$$\n",
    "- important: the above equation is only valid when the eigenvectors are unit-normalized.\n",
    "  - they need to be unit-normalized so that they provide only direction with no magnitude, allowing the magnitude to be specified by the eigenvalue.\n",
    "  - the equation could be genearlized to non-unit vectors by dividing by the magnitudes of the vectors (i.e. eigenvalues).\n",
    "- expanding the summation sign leads to the insight that we are re-expressing diagonalization\n",
    "$$A = V \\Lambda V^T$$\n",
    "- this is slightly different than previous because we previously right multiplied by $V^{-1}$\n",
    "- what does that difference mean?  It means that this equation is only valid for symmetric matrices, because $V^{-1} = V^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- but don't worry, reconstructing a matrix via eigenlayers is still valid for non-symmetric matrices.  We just need a different formulation:\n",
    "$$W = V^{-T}$$\n",
    "$$A = \\sum_{i=1}^{M} v_i \\lambda_i w^T_i$$\n",
    "- now we have the outer product between the eigenvector and the corresponding row of the inverse of the eigenvecot rmatrix transposed, which here is printed as the ith column of matrix $W$.\n",
    "- as opposed to the previous equation, this one does not require unit-normalized eigenvectors.\n",
    "  - why?  it's because this equation includes the matrix inverse.  Thus $VV^{-1}=I$ regardless of the magnitude of the individual eigenvecotrs, whereas $V^TV=I$ only when each eigenvector is unit-normalized.\n",
    "\n",
    "*see example on p. 456*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Who cares about eigenlayers?  It may seem to circuitous to deconstruct a matrix only to reconstruct it again.  But consider this: do you ened to sum up all of the layers?  What if you would sum only the layers witht he largest K > r eigenvalues?  That will actually be a low rank approximation of the original matrix.  Or maybe this is a data matrix and you identify certain eigenvectors that reflect noise; you can then reconstruct the data without the \"noise layers\".  More on this in the next few chapters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.11 Matrix powers and inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One reason eigendecomposition has many applications is that diagonal matrices are really easy to compute with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix powers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- taking matrices to exponential powers can be computationally intensive\n",
    "- but by using eigendecomposition, it becomes computationally simpler:\n",
    "$$A^n = (VDV^{-1})^n = VD^nV^{-1}$$\n",
    "- Matrix powers and eigenvalues:\n",
    "For eigenvalue/vector $\\lambda v$ of matrix $A$,\n",
    "$$A^n v = \\lambda^n v$$\n",
    "\n",
    "*see proof for this on p. 458-459*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the other application follows the same logic:\n",
    "  - diagonalize a matrix, apply some operation to the diagonal elements of $\\Lambda$, then reassemble the 3 matrices into one\n",
    "- recall from Ch. 12 that the inverse of a diag matrix is the diag elements individually inverted.\n",
    "- That's the key insight to inverse-via-eigendecomposition:\n",
    "$$A^{-1} = (VDV^{-1})^{-1} = (V^{-1})^{-1}D^{-1}V^{-1} = VD^{-1}V^{-1}$$\n",
    "- of course, this is only valid for matrices with all non-zero diagonal elements, which excludes all singular matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you may wonder if this is a shortcut considering that $V$ still needs to be inverted, but there are 2 advantages:\n",
    "1. inverting a symmetric matrix (with an orthogonal eigenvectors matrix) where $V^{-1} = V^T$\n",
    "2. because the eigenvectors are normalized, $V$ has a low condition number and is therefore numerically stable.\n",
    "- thus, the $V$ of a non-symmetric matrix might be easier to invert than $A$\n",
    "- *also, it helps build intuition for the algorithm to compute the pseudoinverse via the SVD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.12 Generalized eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- these two equations are equivalent:\n",
    "$$Av = \\lambda v$$\n",
    "$$Av = \\lambda I v$$\n",
    "- but what if we replace $I$ with another (suitably sized) matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalized eigenvalue equation:**\n",
    "$$Av = \\lambda B v$$\n",
    "$$AV = BV \\Lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generalized eigendecomposition is also called *simultaneous diagonalization of two matrices* and leads to several equations that are not immediately easy to interpret including:\n",
    "$$V^{-1}B^{-1}AV = \\Lambda$$\n",
    "$$A = BV\\Lambda V^{-1}$$\n",
    "$$B = AV\\Lambda^{-1}V^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- perhaps a beter way to interpret generalized eigendeomposition is to think about \"regular\" eigendecomposition on a matrix product involving an inverse\n",
    "\n",
    "**Interpretation of generalized eigendecomposition:**\n",
    "$$(B^{-1}A)v = \\lambda v$$\n",
    "$$Cv = \\lambda v, \\;\\;\\; C = B^{-1}A$$\n",
    "- this interpretation is valid only when $B$ is invertible.\n",
    "  - in practice, even when $B$ is invertible, inverting large or high-conditioned matrices can lead to numerical inaccuracies and therefore should be avoided.\n",
    "  - Nonetheless, this equation helps build intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "- Generalized eigendecomposition is easy to implement, just be mindful of the order of function inputs.\n",
    "- using the above equation, $A$ must be the first input and $B$ must be the second input.\n",
    "- Numpy cannot perform generalized eigendecomposition, but scipy can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21736038+0.j        , -0.76862007+1.85331647j,\n",
       "       -0.76862007-1.85331647j])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.linalg import eig\n",
    "n = 3\n",
    "A = np.random.randn(n, n)\n",
    "B = np.random.randn(n, n)\n",
    "evals, evecs = eig(A, B)\n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87423536+0.j        , -0.40604376+0.12059648j,\n",
       "        -0.40604376-0.12059648j],\n",
       "       [ 0.48475644+0.j        , -0.49274146-0.32303729j,\n",
       "        -0.49274146+0.32303729j],\n",
       "       [-0.02690235+0.j        , -0.68128702+0.09636241j,\n",
       "        -0.68128702-0.09636241j]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "You can also think of $B^{-1}A$ as the matrix version of a *ratio* of $A$ to $B$.  This interpretation makes generalized eigendecomposition a computational workhorse for several multivariate data science and machine-learning applications, including linear discriminant analysis, source separation, and classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.13 - 15.14 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do in group discussion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.15 - 15.16 Code Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do in group discussion?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
