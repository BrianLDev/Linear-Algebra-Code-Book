{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: pp. 421 - 462\n",
    "- exercises: pp. 463 - 470"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended supplementary videos:\n",
    "- [Eigenvalues and Eigenvectors, Imaginary and Real](https://youtu.be/8F0gdO643Tc) - Physics by Eugene K\n",
    "- [Eigenvectors and eigenvalues | Chapter 14, Essence of linear algebra](https://youtu.be/PFDu9oVAE-g) - 3 Blue 1 Brown\n",
    "- [A quick trick for computing eigenvalues | Chapter 15, Essence of linear algebra](https://youtu.be/e50Bj7jn9IQ) - 3 Blue 1 Brown\n",
    "- [21. Eigenvalues and Eigenvectors](https://youtu.be/cdZnhQjJu4I) - Gilbert Strang (MIT Open Courseware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 15.1 What are eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are myriad explanations of eigenvectors and eigenvalues, and most students find most explanations incoherent on first impression.\n",
    "- In this section, Cohen provides 3 explanations that he hopes will build intuition, with additional insights to come later.\n",
    "- notation:  typically eigenvalues are labeled $\\lambda$ and eigenvectors are labeled $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key properties of eigendecomposition:\n",
    "1. Eigendecomposition is defined only for square matrices.  They can be singular or inverible, symmetric or triangle or diagonal; but eigendecomposition can only be performed on square matrices.\n",
    "2. The purpose of eigendecomposition is to extract two sets of features from a matrix: eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an MxM matrix has M eigenvalues and M eigenvectors.\n",
    "- The eigenvalues and eigenvectors are paired 1 to 1.\n",
    "- Importantly, eigenvectors/values are not special properties of the vector alone, nor of the matrix alone.  They are a combination of a particular matrix, a particular vector, and a particular scalar.\n",
    "  - changing any of these qualities (even a single element) will likely destroy this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue equation\n",
    "$$Av = \\lambda v$$\n",
    "- this equation is saying that the effect of multiplying the matrix by the vector is the same as scaling the vector\n",
    "- *note that you cannot divide both sides by $v$, because vector division is undefined.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way to think about matrix-vector multiplicaiton is that matrices act as input-output transformers.\n",
    "- Vector $w$ goes in, and vector $Aw=y$ comes out.\n",
    "- The majority of the time, the resulting vector $y$ will point in a different direction from $w$.\n",
    "- in other words, $A$ rotates the vector.\n",
    "- eigenvectors are the unique case where matrix transformation **does not** rotate the matrix (only scales it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we plot data and draw a trendline, it turns out that line is an eigenvector of the data matrix times its transpose, which is also called a covariance matrix.\n",
    "- these lines can be called the \"principal components\" of a matrix\n",
    "- Principal Components Analysis (PCA) is one of the most important tools in data science (e.g. unsupervised machine learning), and it is nothing more than an eigendecomposition of a data matrix.\n",
    "  - more on PCA in chapter 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubik's cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- think of a Rubuk's cube as a matrix (technically it's a tensor but just go with it).\n",
    "- the information in the cube is scattered around, likewise, patterns of info in a data matrix are often distrubuted across rows and columns.\n",
    "- To solve the cube, you perform rotations on the rows and columns\n",
    "- This specific sequence of rotations is like the eigenvectors: they provide a set of instructions for how to rotate the info in the matrix.\n",
    "- Once you apply all the rotations, the info in the matrix becomes \"ordered\" with all of the similar info packed into one eigenvalue.  Thus, the eigenvalue is analogous to a color.\n",
    "- The completed Rubik's cube is analogous to a procedure called \"diagonalization\" which means to put all of the eigenvectors into a matrix, and all of the eigenvalues into a diagonal matrix.  That diagonal matrix is like the solved Rubik's cube.\n",
    "- *(if the Rubik's cube analogy isn't helpful, just ignore it and use the previous interpretations)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Finding eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigenvectors are like secret passages that are hidden inside the matrix.  To find those secret passages, we need to find the secret keys.  Eigenvalues are those keys.\n",
    "- Thus, eigendecomposition requires first finding the eigenvalues, then using those eigenvalues as \"magic keys\" to unlock the eigenvectors.\n",
    "- To find the eigenvalues of a matrix is to re-write the Eigenvalue equation so that we have some expression equal to the zeros vector.\n",
    "$$Av - \\lambda v = 0$$\n",
    "- since $v$ is a shared component, we can factor it out, but we need to insert the identity matrix after $\\lambda$\n",
    "$$Av - \\lambda I v = 0$$\n",
    "$$(A - \\lambda I) v = 0$$\n",
    "- this equation is familiar: it is the same as the definition of the null space from 8.6.\n",
    "- Thus, we've discovered that when shifting a matrix by an eigenvalue, the eigenvector is in its null space.\n",
    "- That becomes the mechanism for finding the eigenvector, but it's all very theoretical at this point--we still don't konw how to find $\\lambda$!\n",
    "- The key here is to remember what we know about a matrix with a non-trivial null space, in particular, about its rank:\n",
    "  - we know that any square matrix with a non-trivial null space is reduced rank.\n",
    "  - we konw that a reduced rank matrix has a determinant of zero.\n",
    "- this leads to the equation for finding the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation for finding eigenvalues (15.5)\n",
    "$$det(A - \\lambda I) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In 11.3 we learned that the determinant of a matrix is computed by solving the characteristic polynomial, and we saw examples of how a known determinant can allow us to solve for some unknown variable inside the matrix.\n",
    "- That's the situation we have here:\n",
    "  - we have a matrix with 1 missing parameter ($\\lambda$) and we know that its determinant is zero.\n",
    "- and that's how you find the eigenvalues of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues of a 2x2 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for a 2x2 matrix, the characteristic polynomial is a quadratic equation.\n",
    "\n",
    "$$det(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}) = 0$$\n",
    "$$det(\\begin{bmatrix} a - \\lambda & b \\\\ c & d - \\lambda \\end{bmatrix}) = 0$$\n",
    "$$(a - \\lambda)(d - \\lambda) - bc = 0$$\n",
    "$$\\lambda^2 - (a + d)\\lambda + (ad - bc) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- since this is a 2nd degree algebraic equation, there are two $\\lambda$ solutions.\n",
    "- The solutions can be found with the quadratic equation (refresher):\n",
    "$$\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n",
    "\n",
    "*see page 430 for example of finding eigenvalues on 2x2 matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slight shortcut for eigenvalues of 2x2 matrix\n",
    "$$\\lambda - tr(A)\\lambda + det(A) = 0$$\n",
    "\n",
    "*(you still need to solve for $\\lambda$ so it isn't the best shortcut, ub it will get you to the characteristic polynimial slighly faster)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues of a 3x3 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The algebra gets more complicated, but the principle is the same: shift the matrix by $-\\lambda$ and solve for $\\Delta = 0$\n",
    "- the characteristic polynomial produces a 3rd order equation here, so there will be 3 eigenvalues as roots for the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$det(\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}) = 0$$\n",
    "$$det(\\begin{bmatrix} a-\\lambda & b & c \\\\ d & e-\\lambda & f \\\\ g & h & i-\\lambda \\end{bmatrix}) = 0$$\n",
    "$$(a - \\lambda)(e - \\lambda)(i - \\lambda) + bfg + cdh - c(e - \\lambda)g - bd(i - \\lambda) - (a - \\lambda)fh = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M columns, M $\\lambda$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- per the fundamental theorem of algebra, any m-degree polynomial has m solutions.\n",
    "- thus, an MxM matrix has an Mth order polynomial, which has M roots, and M eigenvalues.\n",
    "- **an MxM matrix has M eigenvalues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "- eigenvalues have no *intrinsic* sorting.\n",
    "- we can come up with *sensible* sorting.\n",
    "  - e.g. ordering eigenvalues according to their position on the number line or magnitude (distance from zero)\n",
    "  - or by a property of their corresponding eigenvectors.\n",
    "- Sorted eigenvalues can facilitate data analyses, but eigenvalues are an intrinsically unsorted set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Finding eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The eigenvectors of a matrix reveal important \"directions\" in that matrix.\n",
    "- you can think of those directions as being invariant to rotations.\n",
    "- The eigenvectors are encrypted inside the matrix, and each eigenvalue is the decryption key for each eigenvector.\n",
    "- Once you have the key, put it in the matrix, turn it, and the eigenvector will be revealed.\n",
    "- In particular, once you've identified the eigenvalues, shift the matrix by each $-\\lambda_i$ and find a vector $v_i$ in theh null space of that shifted matrix.\n",
    "  - this is the eigenvector associated with eigenvalue $\\lambda_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equations for finding eigenvectors**\n",
    "\n",
    "two methods:\n",
    "$$(A - \\lambda_i I)v_i = 0$$\n",
    "$$v_i(A - \\lambda_i I), \\;\\;\\;\\;\\; v_i \\in N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*see p. 433 for examples of finding eigenvectors in 2x2 matrix*\n",
    "\n",
    "- Interestingly, although eigenVALUES are unique, there are an infinite number of eigenVECTORS that come out of the equation (all scaled versions of the same vector of course)\n",
    "- Thus, **the true interpretation of an eigenvector is a basis vector for a 1D subspace**\n",
    "  - i.e. the \"preferred\" eigenvector is the unit-length basis vector for the null space of the matrix shifted by its eigenvalue.\n",
    "  - computer software like MATLAB and Python libraries will provide the unit length eigenvector\n",
    "  - that said, it can be hard/messy to calculate by hand, so typically when solving problems by hand its easier to use integer elements.\n",
    "\n",
    "*see p. 435 for example of finding eigenvectors in 3x3 matrix*\n",
    "- the process is essentially the same as 2x2 matrix, only a bit more challenging since you are dealing with a 3rd order equation and the determinant calc for a 3x3 matrix is slightly more involved.\n",
    "  - i.e. the process itself is relatively easy to understand but the work can be tedious.\n",
    "- so do a few by hand to internalize the process, then in the future you can let the computer do the work for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 Diagonalization via eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that we have eigenvalues and eigenvectors, we can step back and look at the big picture.\n",
    "- each eigen-equation can separately be listed as:\n",
    "$$Av_1 = \\lambda_1 v_1$$\n",
    "$$Av_2 = \\lambda_2 v_2$$\n",
    "$$...$$\n",
    "$$Av_m = \\lambda_m v_m$$\n",
    "- This is clunky and ugly, but fortunately we can clean it up and make it compact by doing the following:\n",
    "  - Have each eigenvector be a column in a matrix.\n",
    "  - Have each eigenvalue be an element in a diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an equation this is represented as:\n",
    "\n",
    "$$AV = V\\Lambda$$\n",
    "\n",
    "Notes about the equation:\n",
    "- $V$ is the matrix containing the eigenvectors, each column being a separate eigenvector\n",
    "- $\\Lambda$ is the diagonal matrix containing all eigenvalues\n",
    "  - the $\\Lambda$ symbol is the capitalized version of $\\lambda$ in Greek\n",
    "- $\\Lambda$ needs to post-multiply $V$ on the right hand side of the equation ($V \\Lambda$).  Pre-multiply ($\\Lambda V$)will not work.\n",
    "  - explanation as to why can be found on p. 436-437\n",
    "  - 1) we want the eigenvalues to scale each column, not each row.\n",
    "  - 2) if the equation read $AV = \\Lambda V$, then we could multiply both sides by $V^{-1}$, resulting in $A=\\Lambda$ which is typically not true.\n",
    "- remember that each eigenvector must be matched pair-wise with its corresponding eigenvalue\n",
    "  - i.e. if you re-arrange one, make sure you re-arrange the other to maintain pair-wise matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The equation $AV = V\\Lambda$ is not only a practical short-hand for a set of equations; it provides an important conceptual insight into one of the core ideas in eigendecomposition:\n",
    "  - Finding a set of basis vectors such that the matrix is diagonal in that basis space.\n",
    "- This can be seen by left-multiplying both sides of the equation by $V^{-1}$\n",
    "  - (which is a valid operation if we assume that all eigenvectors form an independent set. This happens when there are M distinct eigenvalues)\n",
    "$$V^{-1}AV = \\Lambda$$\n",
    "- thus, matrix $A$ is diagonal in basis $V$.\n",
    "- That's why eigendecomposition is also sometimes called *diagonalization*.\n",
    "- To diagonalize matrix $A$ means to find some matrix of basis vectors such that $A$ is a diagonal matrix in that basis space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Right-multiplying by $V^{-1}$ reveals that a diagonal matrix is \"hidden\" inside each diagonalizable square matrix; the rotation matrix $V$ reveals that hidden organization:\n",
    "$$A = V \\Lambda V^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's revisit the Rubik's cube analogy from earlier in the chapter...\n",
    "- in the equation $V^{-1}AV = \\Lambda$:\n",
    "  - $A$ is the scrambled Rubik's cube with all sides having inter-mixed colors\n",
    "  - $V$ is the set of rotations that you apply to the Rubik's cube in order to solve the puzzle\n",
    "  - $\\Lambda$ is the cube in its \"ordered\" form with each side having exactly one color\n",
    "  - $V^{-1}$ is the inverse of the rotations, which is how you would get from the ordered form to the original mixed form.\n",
    "\n",
    "*see figure 15.5 on p. 438 for a visual example of Diagonalization of a matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "The previous reflection box mentioned sorting eigenvalues, and figure 15.5 shows eigenvalues sorted ascending along the diagonals.  Re-sorting eigenvalues is fine, but you need to be diligent to apply the same re-sorting to the columns of $V$, otherwise the eigenvealues and their associated eigenvectors will be mismatched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  0.],\n",
       "       [ 0., -1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain eigenvalues and eigenvectors in Python\n",
    "import numpy as np\n",
    "A = np.array([[2, 3], [3, 2]])\n",
    "L, V = np.linalg.eig(A)   # outputs a vector for the eigenvalues (L) and a matrix for the eigenvectors (V)\n",
    "L = np.diag(L)            # put eigenvalues in a diagonal matrix\n",
    "# L = np.eye(len(A)) * L    # alternate method to put eigenvalues in a diagonal matrix\n",
    "L   # print the eigenvalue diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678, -0.70710678],\n",
       "       [ 0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V   # print the eigenvectors (unit length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5 Conditions for diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not all square matrices are diagonalizable, only matrices where the equation $A = V\\Lambda V^{-1}$ is true/valid are diagonalizable.\n",
    "- Fortunately, for applied linear algebra, \"most\" matrices are diagonalizable\n",
    "  - i.e. the square matrices that show up in statistics, machine learning, data science, computational simulations, and olter simulations are likely diagonalizable.\n",
    "- **Importantly, all symmetric matrices are diagonalizable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are matrices for which no matrix $V$ can make that decomposition equation true (non-diagonalizable)\n",
    "- Here's an example of a non-diagonalizable matrix:\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "  1 & 1 \\\\\n",
    "  -1 & -1\n",
    "\\end{bmatrix}\n",
    ", \\;\\;\\;\\;\\;\n",
    "\\lambda = \\{0, 0\\}\n",
    ", \\;\\;\\;\\;\\;\n",
    "V = \n",
    "\\begin{bmatrix}\n",
    "  1 & -1 \\\\\n",
    "  -1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- notice that the matrix is rank-1 and yet has two zero-valued eigenvalues.  This means that our diagonal matrix of eigenvalues would be the zeros matrix, and it is impossible to reconstruct the original matrix using $\\Lambda=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilpotent matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an entire category of matrices that is non-diagonalizable, called *nilpotent* matrices.\n",
    "- A nilpotent matrix means that for some matrix power $k$, $A^k=0$\n",
    "  - i.e. keep multiplying the matrix by itself and eventually you'll get the zeros matrix\n",
    "- below is an example of a rank-1 nilpotent matrix with k=2\n",
    "  - confirm for yourself that $AA = 0$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- all triangle matrices that have zeros on the diagonal are nilpotent, and all have zero-valued eigenvalues, and thus cannot be diagonalized.\n",
    "- all hope is not lost, because the singular value decomposition (SVD) is valid on all matrices, even the non-diagonalizable ones.\n",
    "  - SVD is covered in ch: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6 Distinct vs. repeated eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many square matrices have M distinct eigenvalues, which is nice because:\n",
    "- **distinct eigenvalues always lead to distinct (linearly independent) eigenvectors**\n",
    "\n",
    "*see p. 441 - 442 for proof that distinct eigenvalues always lead to distinct eigenvectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Repeated eigenvalues complicate matters because they sometimes have distinct eigenvectors and sometimes not.\n",
    "\n",
    "*see examples on p. 443-444*\n",
    "\n",
    "- now that we've seen examples of the possible outcomes of repeated eigenvalues, the possible outcomes are:\n",
    "  - only one eigenvector or distinct eigenvectors\n",
    "  - an infinity of possible sets of distinct eigenvectors\n",
    "- which of these possibilities depends on the numbers in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric interpretation of repeated eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.7 Complex eigenvalues or eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.8 Eigendecomposition of a symmetric matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9 Eigenvalues of singular matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.10 Eigenlayers of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.11 Matrix powers and inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.12 Generalized eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.13 - 15.14 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.15 - 15.16 Code Challenges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
