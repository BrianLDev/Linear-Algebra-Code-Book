{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: pp. 421 - 462\n",
    "- exercises: pp. 463 - 470"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended supplementary videos:\n",
    "- [Eigenvalues and Eigenvectors, Imaginary and Real](https://youtu.be/8F0gdO643Tc) - Physics by Eugene K\n",
    "- [Eigenvectors and eigenvalues | Chapter 14, Essence of linear algebra](https://youtu.be/PFDu9oVAE-g) - 3B1B\n",
    "- [A quick trick for computing eigenvalues | Chapter 15, Essence of linear algebra](https://youtu.be/e50Bj7jn9IQ) - 3B1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 15.1 What are eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are myriad explanations of eigenvectors and eigenvalues, and most students find most explanations incoherent on first impression.\n",
    "- In this section, Cohen provides 3 explanations that he hopes will build intuition, with additional insights to come later.\n",
    "- notation:  typically eigenvalues are labeled $\\lambda$ and eigenvectors are labeled $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key properties of eigendecomposition:\n",
    "1. Eigendecomposition is defined only for square matrices.  They can be singular or inverible, symmetric or triangle or diagonal; but eigendecomposition can only be performed on square matrices.\n",
    "2. The purpose of eigendecomposition is to extract two sets of features from a matrix: eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an MxM matrix has M eigenvalues and M eigenvectors.\n",
    "- The eigenvalues and eigenvectors are paired 1 to 1.\n",
    "- Importantly, eigenvectors/values are not special properties of the vector, or of the matrix.  They are a combination of a particular matrix, a particular vector, and a particular scalar.  (i.e. they are situation dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue equation\n",
    "$$AV = \\lambda v$$\n",
    "- this equation is saying that the effect of multiplying the matrix by the vector is the same as scaling the vector\n",
    "- note that you cannot divide both sides by $v$, because vector division is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way to think about matrix-vector multiplicaiton is that matrices act as input-output transformers.\n",
    "- Vector $w$ goes in, and vector $Aw=y$ comes out.\n",
    "- The majority of the time, the resulting vector $y$ will point in a different direction from $w$.\n",
    "- in other words, $A$ rotates the vector.\n",
    "- eigenvectors are the unique case where matrix transformation **does not** rotate the matrix (only scales it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we plot data and draw a trendline, it turns out that line is an eigenvector of the data matrix times its transpose, which is also called a covariance matrix.\n",
    "- these lines can be called the \"principal components\" of a matrix\n",
    "- Principal Components Analysis (PCA) is one of the most important tools in data science (e.g. unsupervised machine learning), and it is nothing more than an eigendecomposition of a data matrix.\n",
    "  - more on PCA in chapter 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubik's cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- think of a Rubuk's cube as a matrix (technically it's a tensor but just go with it).\n",
    "- the information in the cube is scattered around, likewise, patterns of info in a data matrix are often distrubuted across rows and columns.\n",
    "- To solve the cube, you perform rotations on the rows and columns\n",
    "- This specific sequence of rotations is like the eigenvectors: they provide a set of instructions for how to rotate the info in the matrix.\n",
    "- Once you apply all the rotations, the info in the matrix becomes \"ordered\" with all of the similar info packed into one eigenvalue.  Thus, the eigenvalue is analogous to a color.\n",
    "- The completed Rubik's cube is analogous to a procedure called \"diagonalization\" which means to put all of the eigenvectors into a matrix, and all of the eigenvalues into a diagonal matrix.  That diagonal matrix is like the solved Rubik's cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Finding eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigenvectors are like secret passages that are hidden inside the matrix.  To find those secret passages, we need to find the secret keys.  Eigenvalues are those keys.\n",
    "- Thus, eigendecomposition requires first finding the eigenvalues, then using those eigenvalues as \"magic keys\" to unlock the eigenvectors.\n",
    "- To find the eigenvalues of a matrix is to re-write the Eigenvalue equation so that we have some expression equal to the zeros vector\n",
    "$$Av - \\lambda v = 0$$\n",
    "- since $v$ is a shared component, we can factor it out, but we need to insert the identity matrix after $\\lambda$\n",
    "$$Av - \\lambda I v = 0$$\n",
    "$$(A - \\lambda I) v = 0$$\n",
    "- this equation is familiar: it is the same as the definition of the null space from 8.6.\n",
    "- Thus, we've discovered that when shifting a matrix by an eigenvalue, the eigenvector is in its null space.\n",
    "- That becomes the mechanism for finding the eigenvector, but it's all very theoretical at this point--we still don't konw how to find $\\lambda$!\n",
    "- The key here is to remember what we know about a matrix with a non-trivial null space, in particular, about its rank:\n",
    "  - we know that any square matrix with a non-trivial null space is reduced rank.\n",
    "  - we konw that a reduced rank matrix has a determinant of zero.\n",
    "- this leads to the equation for finding the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation for finding eigenvalues (15.5)\n",
    "$$det(A - \\lambda I) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In 11.3 we learned that the determinant of a matrix is computed by solving the characteristic polynomial, and we saw examples of how a known determinant can allow us to solve for some unknown variable inside the matrix.\n",
    "- That's the situation we have here:\n",
    "  - we have a matrix with 1 missing parameter ($\\lambda$) and we know that its determinant is zero.\n",
    "- and that's how you find the eigenvalues of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues of a 2x2 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for a 2x2 matrix, the characteristic polynomial is a quadratic equation.\n",
    "\n",
    "$$det(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}) = 0$$\n",
    "$$det(\\begin{bmatrix} a - \\lambda & b \\\\ c & d - \\lambda \\end{bmatrix}) = 0$$\n",
    "$$(a - \\lambda)(d - \\lambda) - bc = 0$$\n",
    "$$\\lambda^2 - (a + d)\\lambda + (ad - bc) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- since this is a 2nd degree algebraic equation, there are two $\\lambda$ solutions.\n",
    "- The solutions can be found with the quadratic equation (refresher):\n",
    "$$\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n",
    "\n",
    "*see page 430 for example of finding eigenvalues on 2x2 matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slight shortcut for eigenvalues of 2x2 matrix\n",
    "$$\\lambda - tr(A)\\lambda + det(A) = 0$$\n",
    "\n",
    "*(you still need to solve for $\\lambda$ so it isn't the best shortcut, ub it will get you to the characteristic polynimial slighly faster)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues of a 3x3 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The algebra gets more complicated, but the principle is the same: shift the matrix by $-\\lambda$ and solve for $\\Delta = 0$\n",
    "- the characteristic polynomial produces a 3rd order equation here, so there will be 3 eigenvalues as roots for the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$det(\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}) = 0$$\n",
    "$$det(\\begin{bmatrix} a-\\lambda & b & c \\\\ d & e-\\lambda & f \\\\ g & h & i-\\lambda \\end{bmatrix}) = 0$$\n",
    "$$(a - \\lambda)(e - \\lambda)(i - \\lambda) + bfg + cdh - c(e - \\lambda)g - bd(i - \\lambda) - (a - \\lambda)fh = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M columns, M $\\lambda$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- per the fundamental theorem of algebra, any m-degree polynomial has m solutions.\n",
    "- thus, an MxM matrix has an Mth order polynomial, which has M roots, and M eigenvalues.\n",
    "- **an MxM matrix has M eigenvalues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "- eigenvalues have no *intrinsic* sorting.\n",
    "- we can come up with *sensible* sorting.\n",
    "  - e.g. ordering eigenvalues according to their position on the number line or magnitude (distance from zero)\n",
    "  - or by a property of their corresponding eigenvectors.\n",
    "- Sorted eigenvalues can facilitate data analyses, but eigenvalues are an intrinsically unsorted set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Finding eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The eigenvectors of a matrix reveal important \"directions\" in that matrix.\n",
    "- you can think of those directions as being invariant to rotations.\n",
    "- The eigenvectors are encrypted inside the matrix, and each eigenvalue is the decryption key for each eigenvector.\n",
    "- Once you have the key, put it in the matrix, turn it, and the eigenvector will be revealed.\n",
    "- In particular, once you've identified the eigenvalues, shift the matrix by each $-\\lambda_i$ and find a vector $v_i$ in theh null space of that shifted matrix.\n",
    "  - this is the eigenvector associated with eigenvalue $\\lambda_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equations for finding eigenvectors**\n",
    "\n",
    "two methods:\n",
    "$$v_i \\in N(A - \\lambda_i I)$$\n",
    "$$(A - \\lambda_i I)v_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*see p. 433 for examples of finding eigenvectors*\n",
    "\n",
    "- Interestingly, there are an infinite number of eigenvectors that come out of the equation (all scaled versions of the same vector of course)\n",
    "- Thus, **the true interpretation of an eigenvector is a basis vector for a 1D subspace**\n",
    "  - i.e. the \"preferred\" eigenvector is the unit-length basis vector for the null space of the matrix shifted by its eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 Diagonalization via eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5 Conditions for diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6 Distinct vs. repeated eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.7 Complex eigenvalues or eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.8 Eigendecomposition of a symmetric matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9 Eigenvalues of singular matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.10 Eigenlayers of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.11 Matrix powers and inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.12 Generalized eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.13 - 15.14 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.15 - 15.16 Code Challenges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
