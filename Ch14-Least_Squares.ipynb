{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: pp. 395 - 414\n",
    "- exercises: pp. 415 - 419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import commonly used Python libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The physical and biological world we inhabit is really, really, *really* complicated.\n",
    "- How do we make sense of the complexity?\n",
    "- We make simplified models of the most important aspects of the system under investigation, while ignoring or abstracting away the aspects that are less relevant.\n",
    "- This process leads to a *model*, which is a set of equations that allows scientists to isolate and understand the principles of the system under investigation.\n",
    "- Thus, the goal of building models is not to replicate *exactly* the system under investigation, but instead to identify a simplified and low-dimensional representation that can be understood by humans or simulated by computers.\n",
    "- On the other hand, models should be sufficiently generic that they can be applied to different datasets.  This means that the models must be flexible enough that they can be adjusted to different datasets without having to create a brand new model for each particular dataset.\n",
    "- This is why models contain both *fixed features* and *free parameters*.\n",
    "  - The fixed features are components of the model that the scientist imposes, based on scientific evidence, theories, and intuition.\n",
    "  - Free parameters are varaibles that can be adjusted to allow the model to fit any particular dataset.\n",
    "- This brings us to the primary goal of model-fitting: find values for these free parameters that make the model match the data as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's say we wanted to predict someone's height.\n",
    "- Your hypotehsis is that height is a function of 3 features:\n",
    "  - sex (male/female)\n",
    "  - parents height\n",
    "  - childhood nutrition (1-10 scale)\n",
    "- *obviously what determines height is much more complicated than this but we are trying to simplify and capture the key factors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now construct our model for height:\n",
    "$$h = \\beta_1s + \\beta_2p + \\beta_3n + \\epsilon$$\n",
    "\n",
    "Fixed Features:\n",
    "- $h$ = height\n",
    "- $s$ = sex\n",
    "- $p$ = parents' height\n",
    "- $n$ = childhood nutrition score\n",
    "- $\\epsilon$ = an \"error term\" / \"residual\" that captures all the variance in height that the 3 variables don't explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free Parameters:\n",
    "- $\\beta$ scalars for each of the factors\n",
    "- they are also often called \"weights\" because the larger the scalar, the more influence the fixed feature has on the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This leads to the question of how you find the model parameters that make the model match the data as closely as possible.\n",
    "- Random guessing is a terrible idea\n",
    "- We need an algorithm that will find the best parameters given the data\n",
    "- The most commonly used algorithm for fitting models to data is **linear least-squares**, which we will learn in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 The five steps of model-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Determine an equation or set of equations that comprise your model.\n",
    "- In practice, you would often use existing equations or slight modifications of existing equations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Work the data into the model.\n",
    "- You get existing data from a database or by collecting data in a scientific experiment, or you can simulate your own data.\n",
    "- Each row of the dataset will be 1 equation.\n",
    "- Combining all equations together results in a system of equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Convert the system of equations into a single matrix-vector equation\n",
    "- This is exactly the same concept learned previously: split up the coefficients, variables and constants, and put them into matrices and vectors.\n",
    "- In previous chapters, we would have called this $Ax=b$, but statistics lingo is slightly different.\n",
    "- Here, it is called $X\\beta = y$\n",
    "  - $X$ = matrix of coefficients\n",
    "  - $\\beta$ = the vector of the unknown free parameters that we want to solve for\n",
    "  - $y$ = the vector of constants we are predicting (in the example, this would be height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Fit the model and solve for $\\beta$\n",
    "- which is also called:\n",
    "  - fitting the model\n",
    "  - estimating the parameters\n",
    "  - computing the best-fit parameters\n",
    "  - some other related terminology\n",
    "- the rest of the chapter is focused on step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Statistically evaluate the model\n",
    "- examples of questions to ask:\n",
    "  - is it a good model?\n",
    "  - how well does it fit the data?\n",
    "  - does it generalize to new data or have we over-fit our sample data?\n",
    "  - do all model terms contribute or should some be excluded?\n",
    "- This step is all about inferential statistics, and it produces things like p-values and t-values and F-values.\n",
    "- Step 5 is important for statistical applications, but is outside the scope of this book, and thus won't be covered in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it's unfortunate and needlessly confusing that statistiians and linear algebrists use different terminology for the same concepts.\n",
    "- the info below shows the terminology translation that will be used in this chapter\n",
    "\n",
    "| LinAlg | Stats | Description                                                             |\n",
    "|--------|-------|-------------------------------------------------------------------------|\n",
    "| $Ax=b$ | $X\\beta=y$| General linear model (GLM)                                              |\n",
    "| $A$    | $X$     | Design matrix (columns = independent variables, predictors, regressors) |\n",
    "| $x$    | $\\beta$ | Regression coefficients or beta parameters                              |\n",
    "| $b$    | $y$     | Dependent variable, outcome measure, data                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Linear\" aspect of linear least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- final point is about the term \"linear\" in linear least squares:\n",
    "- linear refers to the way that the parameters aer estimated; it is not a restriction on the model (the model may contain nonlinear terms and interactions).\n",
    "- the restriction for linearity is that the coefficients (the free parameters) scalar-multiply their variables and sum to predict the dependent variable.\n",
    "- basically taht just meanst hat it's possible to transform the system of equtions into a matrix equation.\n",
    "- that restritcion allows us to use linear algebra methods to solve for $\\beta$\n",
    "- there are also nonlinear methods for estimateing parameters in nonlinear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: the following equation contains 2 nonlinearities:\n",
    "$$h = \\beta_1s + \\beta_2 \\sqrt{\\beta_3}p + \\beta_3 n^3 + \\epsilon$$\n",
    "- one nonlinearity is in the regressors ($\\beta_2 \\sqrt{\\beta_3}$)\n",
    "  - this one prevents linear least-squares from fitting this model\n",
    "  - (but there are nonlinear alternatives that can be learned about in a statistics course)\n",
    "- the other is in a predictor ($n^3$)\n",
    "  - this one is no problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Least-squares via left inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While there is only 1 solution to the general linear model equation, we can take multiple different paths/perspectives to reach that same solution.\n",
    "  - algebra (section 14.4)\n",
    "  - geometry (section 14.5)\n",
    "  - row-rection (section 14.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem is simple:\n",
    "  - solve for $\\beta$ in $X\\beta = y$\n",
    "- if $X$ were a square matrix, we could get the solution by pre-multiplying both sides by $X^{-1}$\n",
    "- however, design matrices are tpically tall matrices, because there are far more observations/records than there are variables/features.\n",
    "  - (nearly all statistical applications involve tall design matries)\n",
    "- So now we know what the solution is:\n",
    "  - instead of the full inverse, we use the left-inverse.\n",
    "  - (see 12.6 - Left inverse for tall matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X\\beta = y$$\n",
    "$$X^{-L}X\\beta = X^{-L}y$$\n",
    "$$(X^TX)^{-1}X^T X \\beta = (X^TX)^{-1}X^T y$$\n",
    "$$\\beta = (X^TX)^{-1}X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements for the above equation:\n",
    "  - $X$ must be full column-rank (which means the design matrix contains a linearly independent set of predictor variables)\n",
    "- if the design matrix has repeated columns, or if one predictor variable can be obtained from a linear combination of other predictor variables, then $X$ will be rank-deficient and the solution is not valid.\n",
    "- in statistics lingo, this situation is called \"multicollinearity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When is the solution exact?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- essentially what we are asking is if vector $y$ is in column space of matrix $X$\n",
    "  - they're both in $\\mathbb{R}^M$ because they both have M elements in the columns (i.e. they both have M rows), so that means its a valid question to ask.\n",
    "  - but $y$ has M elements and $X$ has N columns, where N << M (e.g. M = 1000 vs N = 3)\n",
    "- in this example, we are asking whether a specific vector is inside a 3D subspace embedded in ambient $\\mathbb{R^1000}$\n",
    "- the answer is almost always: No, $y \\notin C(X)$\n",
    "- even for a good model, a tiny bit of noise/variance would push vector $y$ out of column space $X$.\n",
    "- What to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The solution is to think about a slightly different question:\n",
    "  - is there a $\\hat{y}$ such that $\\hat{y}$ really is in the column space of $X$?\n",
    "  - then we would call the parameters $\\hat{\\beta}$\n",
    "- obviously some versino of $\\hat{y}$ and $\\hat{\\beta}$ exist, so we want to find the $\\hat{y}$ that comes closest to $y$ as possible.\n",
    "- $\\hat{y}$ could be considered our \"prediction\" vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings us to the full formulation of the general linear model and related equations:\n",
    "$$\\hat{y} = X \\hat{\\beta} y  \\;\\;\\;\\;\\; = X \\beta + \\epsilon$$\n",
    "$$\\hat{y} = y + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\epsilon$ is the residual (the difference between your predicted $y$ and actual $y$)\n",
    "- the better your model fits the data, the closer $y$ is to the column space of $X$, and the smaller $\\epsilon$ will be.\n",
    "- keep in mind that $\\epsilon$ is not in the data that is measured, and it is not in the model that you specify.  It is the difference between your approximated model & predictions vs the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 Least-squares via orthogonal projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imagine an ambient $\\mathbb{R}^M$ space, and a subspace inside that ambient space whose basis vectors are the columns of the design matrix.\n",
    "  - see figure 14.1 on p. 404 for an example in 2D subspace)\n",
    "- The meausured data in $y$ forms a vector in $\\mathbb{R}^M$.\n",
    "- as mentioned in ch 13, it's sometimes easier to conceptualize $y$ as a coordinate in space rather than a vector from the origin.\n",
    "- when working with real data, it's incredibly unlikely that $y$ is exactly in the column space of $X$\n",
    "  - if you do have an exact fit, then the model is likely too complicated, or the thing you are trying to model is too simple.\n",
    "- But the column space *is* important because that is the mathematical representation of our theory of the world.\n",
    "- So from a geometric perspective, what is the coordinate that is as close asa possible to the data vector while still being in the subspace?\n",
    "  - as we learned from Ch 13, the closest point is found on the orthogonal projection of the vector onto the subspace.\n",
    "- That orthogonal projection is vector $\\epsilon = y - X\\beta$, which is orthogonal to the column space of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X^T \\epsilon = 0$$\n",
    "$$X^T (y - X \\beta) = 0$$\n",
    "$$X^Ty - X^TX \\beta = 0$$\n",
    "$$X^TX \\beta = X^Ty$$\n",
    "$$\\beta = (X^TX)^{-1} X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And amazingly (though not surprisingly), we've arrived at the same solution as in the previous section!\n",
    "- We also see that the design matrix $X$ and the residuals vector $\\epsilon$ are orthogonal.\n",
    "- Geometrically that makes sense; statistically, it means that the prediction errors should be unrelated to the model, which is an important quality check of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "- the author's been a bit loose with plus and minus signs.\n",
    "- for example, why is $\\epsilon$ defined as $y - X\\beta$ and not $X\\beta - y$?\n",
    "- sign invariance often rears its confusing head in linear algebra, and in many cases, it turns out that when the sign seems like it's arbitrary, then the solution ends up being the same regardless.\n",
    "- Also, in many cases, there are coefficients floating around that can absorb the signs.\n",
    "- e.g. you could flip the signs of all the *elements* in $\\epsilon$ to turn vector $-\\epsilon$ into $+\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 Least squares via row-reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we're going to derive the least-squares formula again, this time using row-reduction and Gauss-Jordan elimination.\n",
    "- though it may seem redundant, this section will help link concepts across different chapters of the book, and therefore has high conceptual / educational value.\n",
    "- Methodology:\n",
    "  - recall from Ch 10 that we can solve a system of equations by performing row-reduction on a matrix of coefficients agumented by the constants.\n",
    "  - then in ch 12, we saw how to apply that method in computing the matrix inverse.\n",
    "  - let's apply that same concept here to solve for $\\beta$ in our statistical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$rref([X | y]) \\Rightarrow [I | \\beta]$$\n",
    "- translation: calculating the RREF on matrix $X$ augmented by vector $y$ will give us the Identity matrix, augmented by $\\beta$\n",
    "- *note: this equation does not work - details explained below and a better version is derived*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's think through the equation above to see if any problems come up...\n",
    "  - $[X | y]$ is valid because they have the same number of rows\n",
    "  - row reduction is also valid\n",
    "  - but $[X | y]$ will have N + 1 columns and a rank of N + 1 < M.\n",
    "  - thus, instead of $[I | \\beta]$, the actual outcome will be $\\begin{bmatrix}I \\\\ 0 \\end{bmatrix}$\n",
    "  - in other words, a tall matrix with the identity matrix $I_N$ on top and all zeros underneath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that we've identified the problem, we need to rethink our approach\n",
    "- the solution comes from pre-multiplying the general linear model equation by $X^T$:\n",
    "$$X^T X \\beta = X^T y$$\n",
    "- this is often called the \"normal equation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can go back to Gauss-Jordan and get a sensible result:\n",
    "$$rref([X^T X | X^T y]) \\Rightarrow [I | \\beta]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's think about the sizes.\n",
    "  - $X^TX$ will be N x N (feature count x feature count), and thus the augmented matrix will be of size N x ( N + 1)\n",
    "  - in other words, a wide, full row-rank matrix.\n",
    "  - this will give us our desired $\\beta$ on the right.\n",
    "- for more details about why this works, see detailed explanation on p. 406-407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.7 Model-predicted values and residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we know from earlier that $\\epsilon$ is orthogonal to $X$\n",
    "- statistically, $\\epsilon$ is the residual variance in the data that the model cannot account for\n",
    "- we can also write this as follows:\n",
    "$$\\epsilon = X \\beta - y$$\n",
    "- when is the model a good fit to the data?\n",
    "- the smaller the $\\epsilon$ value, the better the model fits the data\n",
    "- in fact, we don't care about the exact values comprising $\\epsilon$, we just care about the norm of $\\epsilon$:\n",
    "$$||\\epsilon||^2 = ||X \\beta - y||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now, we can reframe the goal of model-fitting in a slightly different way:\n",
    "  - find the values in vector $\\beta$ that minimize both sides of the above equation\n",
    "  - this is a standard optimization problem and can be expressed as:\n",
    "$$min \\beta \\; ||X \\beta - y||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the solution to this problem is obtained by computing the derivative and setting that equal to zero:\n",
    "$$0 = \\frac{d}{d \\beta} ||X \\beta - y||^2 = 2X^t (X \\beta - y)$$\n",
    "$$0 = X^TX \\beta - X^T y$$\n",
    "$$X^TX \\beta = X^T y$$\n",
    "$$\\beta = (X^TX)^{-1}X^T y$$\n",
    "- and here we are, back at the same solution to the least-squares that we obtained previously!\n",
    "  - (only using a bit of calculus and optimization this time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.8 Least-squares example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- doing statistics in the real-world is usually not so simple as just following the above steps.\n",
    "- in practice, statisticians will often begin with simple models and then add complexity to the models as it becomes clear that the simple models are niappropriately suited for the data, and according to the available theoretical frameworks.\n",
    "- Here's an example to get a taste of the process..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We begin with a set of numbers in a dataset called $D$.\n",
    "- We assume the order is meaningful, perhaps these are values recorded from a signal made sequentially in time.\n",
    "$$D = {-4, 0, -3, 1, 2, 8, 5, 8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Determine an equation or set of equations that comprise your model.\n",
    "- we start by assuming that the signal has a constant value\n",
    "  - (i.e. our hypothesis at this point is that there is only 1 meangful signal value, and the divergences at each time point are errors / signal noise)\n",
    "- thus, the model is:\n",
    "$$d = \\beta 1 + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Work the data into the model\n",
    "- doing this gives us a series of equations that looks like...\n",
    "$$\n",
    "\\begin{Bmatrix}\n",
    "-4 = \\beta 1 \\\\\n",
    "0 = \\beta 1 \\\\\n",
    "-3 = \\beta 1 \\\\\n",
    "... \\\\\n",
    "8 = \\beta 1\n",
    "\\end{Bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert the system of equations into a single matrix-vector equation\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "... \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "[\\beta]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-4 \\\\\n",
    "0 \\\\\n",
    "-3 \\\\\n",
    "... \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Fit the model and solve for $\\beta$\n",
    "$$\\beta = (X^TX)^{-1}X^Ty$$\n",
    "$$\\beta = 8^-1 x 17$$\n",
    "$$\\beta = \\frac{17}{8} = 2.125$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Statistically evaluate the model\n",
    "- *note that formal statistical evaluation is skipped in this example, we'll do informal evaluation instead*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an interesting thing happened here...\n",
    "- we ended up summing all the data points (dot product between the numbers and a vector of 1's) and dividing by the number of elements.  That's literally the arithmetic mean!\n",
    "- so we re-derived the average from a statistics / linear algebra perspective, where our model is that the data are meaningful characterized by a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare predictions to actual and evaluate the errors\n",
    "  - for a visual of the output and error comparison, see figure 14.2 on page 411.\n",
    "  - the model looks pretty awful, to be honest...\n",
    "  - the data clearly shows an upward trend which the model cannot capture.\n",
    "- let's try changing the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model # 2:\n",
    "- instead of predicting a single data value, let's predict that the data values change over the x-axis (i.e. linear trend)\n",
    "- the new model equation in step 3 would look like this:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "... \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "[\\beta]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-4 \\\\\n",
    "0 \\\\\n",
    "-3 \\\\\n",
    "... \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's see what the parameter $\\beta$ is\n",
    "  - *note that we are no longer predicting the average value, we are now predicting a slope*\n",
    "$$\\beta = (X^TX)^{-1}X^T y$$\n",
    "$$\\beta = 204^{-1} x 148$$\n",
    "$$\\beta = \\frac{204}{148} = .7255$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- evaluating the new model:\n",
    "  - see figure 14.3 on p. 412 for a new comparison of predicted vs actual\n",
    "  - it looks better, but still not quote right\n",
    "  - the predicted data are too high (over-estimated) in the beginning and too low (under-estimated) in the end\n",
    "  - the problem here is that the model lacks an *intercept term*\n",
    "  - the intercept is the expected value when all other parameters are set to 0\n",
    "    - recall the old $y = mx + b$ formula for a line, where b = the y-intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model # 3:\n",
    "- now let's expand the model to include an intercept term, which involves including an extra column of 1's in the design matrix\n",
    "- we'll also need two $\\beta$s instead of one\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 2 \\\\\n",
    "1 & 3 \\\\\n",
    "... \\\\\n",
    "1 & 8\n",
    "\\end{bmatrix}\n",
    "[\\beta_1]\n",
    "[\\beta_2]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-4 \\\\\n",
    "0 \\\\\n",
    "-3 \\\\\n",
    "... \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the arithmetic gets a bit more involved, but produces the following:\n",
    "$$\\beta = (X^TX)^{-1}X^T y$$\n",
    "$$\\beta = \\begin{bmatrix}8 & 36 \\\\ 36 & 204 \\end{bmatrix}^{-1} \\begin{bmatrix} 17 \\\\ 148 \\end{bmatrix}$$\n",
    "$$\\beta = \\begin{bmatrix} -5.5357 \\\\ 1.7024  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- evaluating the results of the 3rd model:\n",
    "  - looking at figure 14.4 (page 414), we can see that this model fits the data reasonably well.\n",
    "  - there are still some residuals that the model doesn't capture, but they could be random fluctuations.\n",
    "  - notice that the line of best fit doesn't go through the origin anymore.  This is because we added a y-intercept, which turned out to be -5.5357."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the final thing we want to do is confirm that $\\epsilon \\perp X \\beta$\n",
    "- the two columns below show the residual and predicted values, truncated at 4 digits:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\epsilon  & X \\beta \\\\\n",
    "0.1667 & -3.8333 \\\\\n",
    "-2.1310 & -2.1310 \\\\\n",
    "2.5714 & -0.4286 \\\\\n",
    "... & ... \\\\\n",
    "0.0833 & 8.0833\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "$$\\epsilon^T(X\\beta) = 0.000000000000142$$\n",
    "- it may seem like the dot product is not exactly zero, but it is tiny enough to be considered zero plus computer rounding error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.9 - 14.10 Code Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do with group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
