{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python library imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading pp. 141-175  \n",
    "Exercises pp. 176-185\n",
    "\n",
    "- There are several ways of multiplying matrices (all explained in this chapter)\n",
    "- not all pairs of matrices can be multiplied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 \"Standard\" matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notation-wise, this book uses two matrices next to each other (like this: $AB$) to indicate \"standard\" matrix multiplication*\n",
    "\n",
    "- Matrix multiplication is not commutative.  $AB$ != $BA$\n",
    "- to emphasize this, sometimes people use the phrases \"A left-multiplies B\" or \"A pre-multiples B\"\n",
    "- Only matrices where the \"inner dimensions\" are equal in size can be multiplied (e.g. [4,2] x [2,6])\n",
    "- the resulting product matrix's size will be the \"outer dimensions\" (e.g. [4,2] x [2,6] = [4,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can use this new info to explain the previous notation for dot product and outer product\n",
    "- $v^Tw$ for dot product and $vw^T$ for outer product\n",
    "- *note: this notation is still vague imo, since the dot product of $v^Tw$ is a scalar, while matrix multiplication of $v^Tw$ is a matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.18065911 -1.14654382 -1.6825322   0.1480106  -1.5144497   0.70675351]\n",
      " [-1.2243817  -1.47099013  1.56215408  0.6121936   0.06866105 -1.98875251]\n",
      " [ 0.50836082 -2.16128703 -0.83898472  0.54375653 -1.59364308 -0.48299895]\n",
      " [-1.42475762 -1.86280673  1.80743187  0.75587133 -0.00540544 -2.38554967]]\n"
     ]
    }
   ],
   "source": [
    "# In Python, matrix multiplication uses the @ symbol\n",
    "M1 = np.random.randn(4,2)\n",
    "M2 = np.random.randn(2,6)\n",
    "C = M1 @ M2\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 ways to think about / implement matrix multiplication:\n",
    "1. The \"element perspective\"\n",
    "2. The \"layer perspective\"\n",
    "3. The \"column perspective\"\n",
    "4. The \"row perspective\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) The \"element perspective\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each element of the result is the dot product of row of A and column of B\n",
    "- memory tool: left hand --> across the top, right hand vvv across the left = upper left element of product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 important features of matrix multiplication\n",
    "1. The diagonal of the result contains dot products between rows/columns of the same ordinal position (e.g. $a1b1$, $a2b2$, etc)\n",
    "  - *important for understanding data covariance matrices*\n",
    "2. The lower triangle of the result contains dot products between *later* rows in A and *earlier* columns in B (e.g. $a5b1$)\n",
    "3. The upper triangle of the result contains dot products between *earlier* rows in A and *later* columns in B (e.g. $a1b3$)\n",
    "  - *2 & 3 are importand for understanding matrix decompositions like QR decomp and generalized eigendecomposition*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) The layer perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layer perspective involves conceptualizing the product matrix as a series of layers or \"sheets\" that are summed together\n",
    "- implemented by creating outer products by taking the **columns of $A$ multiplied by the rows of $B$** then summing those outer products together\n",
    "- Each outer product is the same size as the result $C$ and can be thought of as a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 4 \\\\\n",
    "-1 & 2 \\\\\n",
    "0 & 4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "5 & 1 \\\\\n",
    "3 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "15 & 3 \\\\\n",
    "-5 & -1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "12 & 4 \\\\\n",
    "6 & 2 \\\\\n",
    "12 & 4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "27 &  7 \\\\\n",
    "1 & 1 \\\\\n",
    "12 & 4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) The column perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- treats all matrices as sets of column vectors, the product matrix is created one column at a time.\n",
    "- the 1st column of the product matrix is a linear weighted combination of all columns in the left matrix, where the weights are defined by the elements in the first column of the right matrix.\n",
    "  - the 2nd column is again a weighted combination of all columns in the left matrix, except that the weights now come from the 2nd column in the right matrix.\n",
    "  - etc\n",
    "- **note that for column perspective, matrix B creates the weights!**\n",
    "- the column perspective is useful in statistics when the columsn of the left matrix contain a set of regressors (a simplified model of the data), and the right matrix contains coefficients (i.e. the importance of each regressor). More on this in ch 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 4 \\\\\n",
    "-1 & 2 \\\\\n",
    "0 & 4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "5 & 1 \\\\\n",
    "3 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "5\n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "-1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+ 3\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "4\n",
    "\\end{bmatrix}\n",
    "\\hspace{0.5cm}\n",
    "1\n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "-1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+ 1\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "4\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "27 &  7 \\\\\n",
    "1 & 1 \\\\\n",
    "12 & 4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) The row perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to column perspective but for rows\n",
    "- each row in the product matrix is the weighted sum of all rows in the right matrix, where the weights are given by elements in each row of the left matrix.\n",
    "  - think of matrix $A$ broken out where each element is a scalar (weight)\n",
    "  - then take the 1st row of $B$ and multiply it by the 1st column of scalars from $A$\n",
    "  - then take the 2nd row of $B$ and multiply it by the 2nd column of scalars from $A$\n",
    "  - etc...\n",
    "- **note that for row perspective, matrix A creates the weights!**\n",
    "- useful in cases like principal components analysis, where the rows of the right amtrix contain data (obaservations in rows and features in columns) and the rows of the left matrix contain weights for combining the features.  Then the weighted sum of data creates the principal component scores.\n",
    "\n",
    "*no example because it's a pain to do in LaTeX*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Multiplication and equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when multiplying both sides of an equation by a scalar, order doesn't matter\n",
    "- but since matrix multiplication is **not commutative** if you are multiplying both sides of an equation by a matrix then you must put them in the **same order** on both sides of the equation.\n",
    "- i.e. if you pre-multiply the left side with matrix $D$ , you must also pre-multiply the right side.  (same for post-multiplyting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4968394   1.0134623 ]\n",
      " [ 0.63156963 -4.24308136]]\n"
     ]
    }
   ],
   "source": [
    "# Confirm in code that A@B != B@A\n",
    "A = np.random.randn(2,2)\n",
    "B = np.random.randn(2,2)\n",
    "C1 = A@B\n",
    "C2 = B@A\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.48533552  0.23818947]\n",
      " [-1.36932077 -0.25458523]]\n"
     ]
    }
   ],
   "source": [
    "print(C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Multiplication with diagonals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a special property of multiplication when one matrix is a diagonal matrix and the other is a dense matrix\n",
    "- pre-multiplication by a diagonal matrix scales the **rows** of the right matrix by the diagonal elements\n",
    "- post-multiplication by a diagonal matrix scales the **columns** of the left matrix by the diagonal elements\n",
    "\n",
    "Mneumonic:  \n",
    "- P**R**e-multiply to affect **R**ows\n",
    "- P**O**st-multiply to affect c**O**columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplying two diagonal matrices\n",
    "- The product of 2 diagonal matrices is another diagonal matrix whose diagonal elements are the products of the corresponding diagonal elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "$$\\begin{bmatrix}\n",
    "a & 0 & 0 \\\\\n",
    "0 & b & 0 \\\\\n",
    "0 & 0 & c\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "d & 0 & 0 \\\\\n",
    "0 & e & 0 \\\\\n",
    "0 & 0 & f\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "ad & 0 & 0 \\\\\n",
    "0 & be & 0 \\\\\n",
    "0 & 0 & cf\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 LIVE EVIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important but not intuitive point:**\n",
    "- An operation applied to multiplied matrices gets applied to each matrix individually **in reverse order**\n",
    "- You will need to swap the order of the matrices before multiplying them\n",
    "- *note: think of it as similar to chained methods in computer programming.  The rightmost method resolves first, then the next rightmost, etc*\n",
    "\n",
    "e.g. $(ABC)^T = C^T B^T A^T$\n",
    "\n",
    "e.g. $(ABCD)^{-1} = D^{-1} C^{-1} B^{-1} A^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- be careful with square matrices because you can still get a result (an incorrect one) if you don't apply LIVE EVIL transpose in correct order\n",
    "- fortunately, it's impossible to multiply rectangular matrices without performing LIVE EVIL transposing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Important feature of matrix-vector multiplication: the result is always a vector\n",
    "- this provides the connection between linear transformations and matrices:\n",
    "  - to apply a transform to a vector, convert the transform to a matrix, then multiply the vector by that matrix\n",
    "- pre-multiplying by a vector gives a different result from post-multiplying by the same vector (transposed to match size)\n",
    "- exception: if the matrix is symmetric, then pre-multipying by a vector is essentially the same as post-multipying by the vector (transposed).\n",
    "  - *Though one is a column vector and the other is a row vector, they're essentially the same*\n",
    "- formally: if $A = A^T$ then $Ab = (b^TA)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Creating symmetric matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 methods for non-symmetric matrices to be converted to symmetric:\n",
    "1) additive\n",
    "2) multiplicative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive method\n",
    "- not widely used but useful to know\n",
    "- Add the matrix to its transpose and divide by 2\n",
    "- *only valid for square matrices*\n",
    "\n",
    "$C = 1/2(A^T+A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative method\n",
    "- more commonly used\n",
    "- multiply a matrix by its transpose (this is the $A^TA$ learned previously)\n",
    "- can use on any matrix, square or non-square\n",
    "\n",
    "$A^TA$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Multiply symmetric matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in general, the product of 2 symmetric matrices is not a symmetric matrix\n",
    "- (there are exceptions, but this is generally the case)\n",
    "\n",
    "Reflection:\n",
    "- this may seem like a useless factoid, but it leads to one of the biggest limitations of principal components analysis, and one of the most important advantages of generalized eigendecomposition, which is the computational backbone of many machine-learning methods, most prominently linear classifiers and discriminant analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Hadamard multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hadamard multiplication is what a layperson would guess matrix multiplication would be like\n",
    "- Simply multiply each element of one matrix with the corresponding element of another matrix\n",
    "- Both matrices must be the same size (row x columns) and the result will be the same size\n",
    "- because Hadamard multiplication is implemented element-wise, it actually is commutative, where \"standard\" matrix multiplication isn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadamard multiplication applications:\n",
    "- one of the key algorithms for computing the matrix inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.14037788 -0.00928369 -0.47585834 -0.14712052]\n",
      " [ 1.5345449   0.42816     0.78747033 -0.85696369]\n",
      " [ 1.92785464 -1.22364961  0.14584775  0.29115969]]\n"
     ]
    }
   ],
   "source": [
    "# Hadamard multiplication in Python (note the * symbol!)\n",
    "M1 = np.random.randn(3,4)\n",
    "M2 = np.random.randn(3,4)\n",
    "print(M1 * M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 Frobenius dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an operation that produces a scalar (single number) given 2 matrices of the same size (M x N)\n",
    "- also called Frobenius inner product\n",
    "- to compute:\n",
    "  - first vectorize the 2 matrices (concatenate all the columns to make a single large column vector)\n",
    "  - then compute their dot product as with normal vectors\n",
    "\n",
    "Notation:  \n",
    "$<A,B>_F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 2, 5, 3, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize a matrix into 1 column in Python\n",
    "# (note that Python defaults to rows so you have to spedify Fortran convention)\n",
    "A = np.array([ [1,2,3],[4,5,6] ])\n",
    "A.flatten(order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A curious but useful way to compute the Frobenius dot product between A and B is by taking the trace of $A^TB$.\n",
    "- Therefore: $<A,B>_F = tr(A^TB)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frobenius dot product has several uses in signal processing in machine learning, for example, as a measure of \"distance\" or similarity between 2 matrices\n",
    "- The Frobenius inner product of a matrix with itself is the sum of all squared elements, and its called the *squared Frobenius norm* or *squared Euclidean norm* of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4493756775317816\n"
     ]
    }
   ],
   "source": [
    "# Compute the Frobenius dot product by using the trace transpose trick\n",
    "A = np.random.randn(3,4)\n",
    "B = np.random.randn(3,4)\n",
    "frob = np.trace(A.T@B)\n",
    "print(frob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10 Matrix norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors:\n",
    "- The norm of a vector is the square root of the dot product of a vector with itself.\n",
    "- This is the same as the magnitude / length of a vector (which is also called the norm)\n",
    "\n",
    "Matrices:\n",
    "- Annoyingly, the norm of a matrix is more complicated.\n",
    "- there are many types of norms but they all have some things in common:\n",
    "  - single number\n",
    "  - corresponds in some way to the \"magnitude\" of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius matrix norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- see eq. 6.24 in book\n",
    "- If we think of a matrix space as Euclidean, then the Frobenius norm of the subtraction of two matrices provides a measure of Euclidean distance between those matrices (i.e. the pythagorean theorem)\n",
    "- only valid for 2 matrices of the same size\n",
    "- sometimes called the ℓ2 norm (cursive l)\n",
    "  - there is also an ℓ1 matrix norm: sum the absolute values of all elements in a column then take the largest maximum column sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know 3 ways to compute the Frobenius norm:\n",
    "1. equation 6.24\n",
    "2. vectorizing the matrix and computing the dot product with itself\n",
    "3. computing $tr(A^TB)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other norms\n",
    "- There are many other matrix norms with varied formulas\n",
    "- to avoid overwhelming, they won't be covered in this book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9486756166769235"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate frobenius norm in Python\n",
    "A = np.random.randn(3,4)\n",
    "np.linalg.norm(A, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11 Matrix assymetry index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **every square matrix can be expressed as the sum of a symmetric matrix and a skew-symmetric matrix**\n",
    "  - (a skew-symmetric matrix can also be called an asymmetric matrix)\n",
    "- quite a claim, and not obvious at first\n",
    "- think of the skew-symmetric matrix as a \"residual\" that we added to the symmetric matrix to produce the new matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*matrix asymmetric index:*\n",
    "- In order to find out how asymmetric a matrix is, we would like to derive a scalar index to quantify it\n",
    "- perfectly symmetric = 0, perfectly skew-symmetric = 1.\n",
    "- computed as the ratio of the norms of asymmetric \"layer\" to the original matrix\n",
    "\n",
    "$A_\\sigma = ||A_k||^2_F / ||A||^2_F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.12 What about matrix division?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matrix division does not exist in general, but there's an equivalent operation\n",
    "- since $\\frac{2}{3} = 2*3^{-1}$, we can perform matrix division with:\n",
    "\n",
    "$AB^{-1}$\n",
    "\n",
    "- This is important enough that it has its own chapter (ch 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.13-6.14 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.15 Code Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
