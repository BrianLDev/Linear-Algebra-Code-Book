{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- content: p. 471 - 502\n",
    "- exercises: p. 503 - 520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Singular Value Decomposition (SVD) is closely related to eigen-decomposition.\n",
    "- In fact, eigendecomposition can be seen as a special case of the SVD, with SVD being the generalized algorithm.\n",
    "  - i.e. eigendecomposition works only on square matrices, SVD works on all matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Core idea of SVD:**\n",
    "- provide a set of basis vectors called *singular vectors* for the 4 matrix subspaces (row space, null space, column space, left-null space).\n",
    "- provide scalar *singular values* that encode the \"importance\" of each singular vetor.\n",
    "  - (Singular vectors are similar to eigenvectors, singular values are similar to eigenvalues.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equation for Singular Value Decomposition (SVD):**\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "$A$ = The MxN matrix to be decomposed.  It can be square or rectangular, and any rank.\n",
    "\n",
    "$U$ = The *left singular vectors matrix* (MxM), which provides an orthonormal basis for $\\mathbb{R}^M$.  This includes the column space of $A$ and its complementary left-null space.\n",
    "- The size of $U$ corresponds to the number of rows in $A$ (recall that counter-intuitively, the size of the column space = the number of rows, i.e. the count of total elements in each column).\n",
    "\n",
    "$\\Sigma$ = The *singular values matrix* (MxN), which is diagonal and contains the singular values (the ith singular value is indicated $\\sigma_i$).  All singular values are non-negative (that is, positive or zero) and real-valued.\n",
    "- The size of $\\Sigma$ is the same as A.\n",
    "\n",
    "$V$ = The *right singular vectors matrix* (NxN), which provides an orthonormal basis for $\\mathbb{R}^N$.  That includes the row space of $A$ and its complementary null space.\n",
    "- The size of $V$ corresponds to the number of columns in $A$ (recall that counter-intuitively, the size of the row space = the number of columns, i.e. the count of total elements in each row).\n",
    "- Notice that the decomposition contains $V^T$; hence, although the right singular vectors are in the *columns* of $V$, it is usually more convenient to speak of the right singular vectors as being the *rows* of $V^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sizes of SVD matrices**\n",
    "\n",
    "<img src='img/16/SVD-sizes.jpg' alt='SVD sizes' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 Computing the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may think that computing the SVD is very difficult, but the truth is that once you konw eigendecomposition, the SVD is almost trivial to compute.\n",
    "- Start by considering eigendecomposition of matrix $A$ of size $M \\neq N$\n",
    "  - eigendecomposition is not defined for non-square matrix, however $A^TA$ is eigendecomposable.\n",
    "  - Replacing $A^TA$ with the SVD matrices gives us the following:\n",
    "\n",
    "$$A^TA = (U \\Sigma V^T)^T(U \\Sigma V^T)$$\n",
    "$$A^TA = V \\Sigma^T U^TU \\Sigma V^T$$\n",
    "$U$ is orthogonal, ergo $U^TU=I$.  Also $\\Sigma$ is diagonal, so $\\Sigma^T \\Sigma = \\Sigma^2$\n",
    "$$A^TA = V \\Sigma^2 V^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can immediately see why the singular values are non-negative--any real number squared will be non-negative.\n",
    "- we're missing the U matrix, but we can obtain it via the eigendecomposition of matrix $AA^T$:\n",
    "\n",
    "$$AA^T = (U \\Sigma V^T)(U \\Sigma V^T)^T$$\n",
    "$$AA^T = U \\Sigma V^T V \\Sigma^T U^T$$\n",
    "$V$ is orthogonal, ergo $V^TV=I$.  Also $\\Sigma$ is diagonal, so $\\Sigma \\Sigma^T = \\Sigma^2$\n",
    "$$AA^T = U \\Sigma^2 U^T$$\n",
    "\n",
    "- So now we see that the way to compute the SVD of any rectangular matrix is to apply the following steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to compute SVD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the eigendecomposition of $A^TA$ to get $V$ (and $\\Sigma$).\n",
    "2. Compute the eigendecomposition of $AA^T$ to obtain $U$ (and $\\Sigma$).\n",
    "\n",
    "- Note that it's actually not necessary to complete both steps to obtain the SVD.\n",
    "- After completing one step, we can compute the missing matrix by using one of the following formulas:\n",
    "$$A V \\Sigma^{-1} = U$$\n",
    "$$\\Sigma^{-1}U^TA = V^T$$\n",
    "\n",
    "- quick aside: how do we know that $U$ and $V$ are orthogonal matrices?\n",
    "  - because they come from the eigendecomposition of a symmetric matrix.  Look back at ch. 15 eigendecomposition of symmetric matrices for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When first computing the SVD by hand (which the author recommends doing at least a few times to solidify the concept), you should first decide whether to apply step 1 and then solve for U, or apply step 2 and then solve for V.\n",
    "- The best strategy depends on the size of the matrix, because you want to compute the eigendecomposition of whichever of $A^TA$ or $AA^T$ is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing singular vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Singular vectors, like eigenvectors, are important because of their direction, so it may seem unnecessary to normalize them.\n",
    "- But since the singular values are scaling the singular vectors, the vectors must be normalized or else $A \\neq U \\Sigma V^T$.\n",
    "- Therefore, **all singular vectors must be normalized to unit vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also the signs of the singular vectors are not arbitrary; the singular values are all positive, so flipping signs of singular vectors may be necessary in reconstructing the matrix.\n",
    "- This is noticeably different from diagonalization via eigendecomposition, where the eigenvectors are sign and magnitude invariant.\n",
    "  - the key to understanding this difference is that the eigenvalues matrix is flanked on both sides by the eigenvectors matrix and its inverse ($V \\Lambda V^{-1}$); any non-unit-magnitudes in $V$ can be absorbed into $V^{-1}$.\n",
    "  - But in the SVD, $U$ and $V^T$ are not inverses of each other (indeed, they may even have different dimensionalities), and thus the magnitude of singular vectors is not cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3 Singular values and eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The prvious section seemed to imply the trivial relationship that the eigenvalues of $A^TA$ equal the squared singular values of $A$.\n",
    "- That is true, but there is a more nuanced relationship between the eigenvalues and the singular values of a matrix.\n",
    "  - This relationsih pis organized into three cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: eig $(A^TA)$ vs. svd $(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The eigenvalues equal the squared singular values, for the reasons explained in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "3 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "A^TA = \n",
    "\\begin{bmatrix}\n",
    "10 & 4 & 0 \\\\\n",
    "4 & 2 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\\lambda(A^TA) = 0, .3431, 11.6569$$\n",
    "$$\\sigma(A) = .5858, 3.4142$$\n",
    "$$\\sigma^2(A) = .3431, 11.6569$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why are there three $\\lambda$'s but only two $\\sigma$'s?\n",
    "  - It's because $A^TA$ is 3x3 but $\\Sigma$ has the same size as the matrix $A$, which is 2x3; hence, the diagonal has only two elements.  But the non-zero $\\lambda$'s equal the squared $\\sigma$'s.\n",
    "- This case concerns the eigenvalues of $A^TA$, not the eigenvalues of $A$.\n",
    "  - In fact, there are no eigenvalues of $A$ because it is not a square matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: eig $(A^TA)$ vs. svd $(A^TA)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, the eigenvalues and singular values are identical--without squaring the singular values.\n",
    "- This is because eigendecomposition and SVD are the same operation for a square symmetric matrix (more on this point later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3a: eig $(A)$ vs. svd $(A)$ for real-valued $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is different from case 2 because here we assume that $A$ is not symmetric, which means that the eigenvalues can be real-valued or complex-valued, depending on the elements in the matrix.\n",
    "- We start by considering the case of a matrix with all real-valued eigenvalues.\n",
    "  - of course, the matrix does not need to be square for it to have eigenvalues, so let's add another row to the previous example above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "3 & 1 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\\lambda(A) = .5858, 1, 3.4142$$\n",
    "$$\\sigma(A) = .4938, 1.1994, 3.6804$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no easy to spot relationship between the eigenvalues and the singular values.  In fact, there isn't really a relationship at all.\n",
    "- Of course there is a macro relationship that $W \\Lambda W^{-1} = U \\Sigma V^T$ (i.e. the entire eigendecomposition equals the entire SVD since they are both equal to A)\n",
    "  - But the macro relationship does not mean that there is any relationship between $\\Lambda$ and $\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3b: eig $(A)$ vs. svd $(A)$ for complex-valued $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The lack of an obvious relationship between eigenvalues and singular values is even more apparent when a matrix has complex-valued eigenvalues.\n",
    "- We know that a real-valued matrix can have complex-valued eigenvalues, but what do we know about singular values?\n",
    "- **The singular value of all matrices--real or complex--are guaranteed to have real-valued singular values.**\n",
    "- Why?  Because the SVD can be obtained from the eigendecomposition of the matrix times its transpose, and that matrix is always symmetric.\n",
    "  - *(The SVD of complex matrices uses the Hermitian transpose instead of the regular transpose)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cases 2 and 3 may initially seem contradictory.\n",
    "- $A^TA$ is simply a matrix so if we set $C=A^TA$, then we've written that $\\lambda(C)=\\sigma(C)$ but $\\lambda(A) \\neq \\sigma(A)$.\n",
    "- care to guess why?\n",
    "  - The difference is that $C$ is defined as a matrix times its transpose, whereas $A$ is not.  And a matrix times its transpose has definite properties that a normal matrix may not have (i.e. square, symmetric, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix \"energy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigendecomposition and SVD are exact decompositions, which means that all the \"energy\" contained in matrix $A$ must be contained inside the three eigendecomposition / SVD matrices.\n",
    "  - For SVD, matrices $U$ and $V$ are orthogonal and have a matrix norm of 1, which means that **all the \"energy\" is contained in matrix $\\Sigma$.\n",
    "  - Eigendecomposition, on the other hand, has an orthogonal eigenvectors matrix only when the matrix is symmetric.  When it is non-symmetric, then the \"total energy\" in the matrix can be distributed over the eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In conclusion, there is a clear relationship between the eigenvalues of $A^A$ and the singular values of $A$ (or the singular values of $A^TA$), but there is no relationship between the eigenvalues of non-symmetric $A$ adn the singular values of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "The SVD is very easy to compute in Python.\n",
    "- *note that Python returns $V^T$ whereas MATLAB returns $V$.*\n",
    "- Python also returns the singular values in a vector instead of in a diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = [[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "\n",
      "s = [1.73205081 1.        ]\n",
      "\n",
      "V = [[-4.08248290e-01 -8.16496581e-01 -4.08248290e-01]\n",
      " [-7.07106781e-01  2.13278616e-16  7.07106781e-01]\n",
      " [ 5.77350269e-01 -5.77350269e-01  5.77350269e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [[1, 1, 0], [0, 1, 1]]\n",
    "A = np.array(a)\n",
    "U, s, V = np.linalg.svd(A)\n",
    "print(\"U = {}\\n\".format(U))\n",
    "print(\"s = {}\\n\".format(s))   # note that s is a vector and not a diagonal matrix\n",
    "print(\"V = {}\".format(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.73205081 0.        ]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "S = np.diag(s)  # convert s --> S as a diagonal matrix\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.4 SVD of a symmetric matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5 SVD and the four subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6 SVD and matrix rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7 SVD spectral theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.8 SVD and low-rank approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9 Normalizing singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.10 Condition number of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD and the matrix inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.12 The MP Pseudoinverse, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.13 - 16.14 Code Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In Chapter 13, you learned about \"economy\" QR decomposition, which can be useful for large tall matrices. There is a comparable \"economy\" version of the SVD. Your goal here is to figure out what that means. First, generate three random matrices: square, wide, and tall. Then run the full SVD to confirm that the sizes of the SVD matrices match your expectations (e.g., Figure 16.1). Finally, run the economy SVD on all three matrices and compare the sizes to the full SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtain the three SVD matrices from eigendecomposition, as described in section 16.2. Then compute the SVD of that matrix using the svd () function, to confirm that your results are correct. Keep in mind the discussions of sign-indeterminacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write code to reproduce panels $\\mathrm{B}$ and $\\mathrm{C}$ in Figure 16.5. Confirm that the reconstructed matrix (third matrix in panel C) is equal to the original matrix. (Note: The matrix was populated with random numbers, so don't expect your results to look exactly like those in the figure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a random-numbers matrix with a specified condition number. For example, create a $6 \\times 16$ random matrix with a condition number of $\\kappa=42$. Do this by creating random $\\mathbf{U}$ and $\\mathbf{V}$ matrices, an appropriate $\\boldsymbol{\\Sigma}$ matrix, and then create $\\mathrm{A}=\\mathbf{U \\Sigma V}^{\\mathrm{T}}$. Finally, compute the condition number of $\\mathrm{A}$ to confirm that it matches what you specified (42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. This and the next two challenges involve taking the SVD of a picture. A picture is represented as a matrix, with the matrix values corresponding to grayscale intensities of the pixels. We will use a picture of Einstein. You can download the file at https://upload.wikimedia.org/wikipedia/en/8/86/Einstein_tongue.jpg of course, you can replace this with any other picture a selfie day... However, you may need to apply some image pedding to reduce the image matrix from $3 \\mathrm{D}$ to $2 \\mathrm{D}$ (thus, processing stead of RGB) and the datatype must be double (MATLAB) or floats (Python).\n",
    "\n",
    "After importing the image, construct a low-rank approximation using various numbers of singular values. Show the original and low-rank approximations side-by-side. Test various numbers of components and qualitatively evaluate the results. Tip: You don't need to include the top components!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a scree plot of the percent-normalized singular values. Then test various thresholds for reconstructing the picture (e.g., including all components that explain at least $4 \\%$ of the variance). What threshold seems reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The final challenge for this picture-SVD is to make the assessments of the number of appropriate components more quantitative. Compute the error between the reconstruction and the original image. The error can be operationalized as the RMS (root mean square) of the difference map. That is, create a difference image as the subtraction of the original and low-rank reconstructed image, then square all matrix elements (which are pixels), average over all pixels, and take the square root of that average. Make a plot of the RMS as a function of the number of components you included. How does that function compare to the scree plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What is the pseudoinverse of a column vector of constants? That is, the pseudoinverse of $k 1$. It obviously doesn't have a full inverse, but it is clearly a full column-rank matrix. First, work out your answer on paper, then confirm it in MATLAB or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. The goal here is to implement the series of equations on page 505 and confirm that you get the same result as with the pinv() function. Start by creating a $4 \\times 2$ matrix of random integers between 1 and 6 . Then compute its SVD (Equation 16.29). Then implement each of the next four equations in code. Finally, compute the MP pseudoinverse of the tall matrix. You will now have five versions of the pseudoinverse; make sure they are all equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. This challenge follows up on the first code challenge from the previous chapter (about generalized eigendecomposition implemented as two matrices vs. the product of one matrix and the other's inverse). The goal is to repeat the exploration of differences between eig $(A, B)$ and eig $(\\operatorname{inv}(B) * A)$. Use only $10 \\times 10$ matrices, but now vary the condition number of the random matrices between $10^{1}$ and $10^{10}$. Do you come to different conclusions from the previous chapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. This isn't a specific code challenge, but instead a general suggestion: Take any claim or proof I made in this chapter (or any other chapter), and demonstrate that concept using numerical examples in code. Doing so (1) helps build intuition, (2) improves your skills at translating math into code, and (3) gives you opportunities to continue exploring other linear algebra principles (I can't cover everything in one book!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "656cd6254c7b2065e00f01cfa19c07ca066e7d85ef474593a41f6dcc31570de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
